<!DOCType>
<html>
<head>
	<title>Chat Transcript</title>
</head>
<style type="text/css">
	p.Jaap
	{
		background: ccccff;
		
	}
	p.Mike
	{
		background: ffffcc;
	}
</style>
<body>
	<p class="Mike">
		Ah, I'd say computers ara teeny, tiny part of what I'm refering to as amplification.
	</p>
	<p class="Mike">
		I think we may have a misunderstanding there.
	</p>
	<p class="Mike">
		When I say amplification, I mean very specifically the ability the human brain exhibits
		to take a small signal, and quicky pass it to millions of different neurons for
		parallel processing. This is a quality not currently found in computers, which are
		mainly good at "do this, then do this" kind of jobs--even if they perform those
		tasks blazingly fast.
	</p>
	<p class="Mike">
		My argument is that parallelization on the scale of the human brain, while in principle
		implementable in a single-core processor, is terribly intractible.
	</p>
	<p class="Jaap">
		Hey? Remember you mentioned the case of atom bomb scientists investigating the possibility
		of the atmosphere igniting?</p>
	</p><p class="Jaap">
		It's discussed by Eliezier here: http://lesswrong.com/lw/rg/la602_vs_rhic_review/
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Mike">
		"AFAICT (I am not a physicist) a Castle Bravo type oversight could not realistically
		have made the atmosphere ignite anyway, and if it did, it'd have gone right out,
		etc."
	</p>
	<p class="Mike">
		Not sure how he can say that. The Castle Bravo oversight was specifically due to
		something the scientists knew nothing about.
	</p>
	<p class="Jaap">
		He can say it as far as he can tell. That's what AFAICT means, especially with the
		not-a-physicist diclaimer. How confident are you that the observation you're sharing
		here about a possible incorrect statement by Eliezier does more than X percent damage
		to the overal point of the article, and doesn't suffer from kill-your-idols bias?
	</p>
	<p class="Mike">
		Eh?
	</p>
	<p class="Mike">
		I think you've misinterpreted what I said.
	</p>
	<p class="Jaap">
		Sorry, I'm inclined to interpret your corrections to Eliezier's writings at a you're-nitpicking
		level based on past comments you've about the cult of Eliezier (which I readily
		admit does exist for some cult-ish metric)
	</p>
	<p class="Mike">
		All I'm saying is, the Castle Bravo event was due to things nobody knew about. It
		is simply not realistic to expect the authors of LA-602 to have taken account of
		things they didn't know about. This doesn't require extensive physics knowledge.
	</p>
	<p class="Mike">
		As far as the rest of the article, I'm currently reading it.
	</p>
	<p class="Jaap">
		I don't think I'd become long term beer drinking friends with Eliezier very soon,
		but I think he's above averagely constructively working on a noble goal.
	</p>
	<p class="Jaap">
		Ok, point taken. Sorry.
	</p>
	<p class="Jaap">
		I'm a little sensitive as of late by any remarks that I believe are biased solely
		due to author association. It comes up a lot in vaccines debate, from either side....
	</p>
	<p class="Jaap">
		Hey, have we talked about front-loading sentences before?
	</p>
	<p class="Mike">
		Hmm, the term seems familiar.
	</p>
	<p class="Jaap">
		Where in interaction with a particular person, you know they're inclined to interrupt
		you to point out a flaw in your reasoning, and that therefore you have to frontload
		all the constraints necessary for your point to hold. You can't make them afterwards,
		because you'll be interrupted already. And you can't say then; I was about to say
		that...
	</p>
	<p class="Jaap">
		The risk is, frontload too much, and the person you're talking to will no longer
		be able to juggle all the constraints in his or her short term active memory.
	</p>
	<p class="Jaap">
		Especially because those constaints lack the binding context (which would make storage
		easier) of the point you're trying to make.
	</p>
	<p class="Jaap">
		That's one reason I love the written word, because then you _can_ actually defer
		constraints to later, without running the risk of somebody pointing out that you're
		wrong.
	</p>
	<p class="Jaap">
		It's quite plausible (to me anyway) there is a strongly *inverse* correlation to
		a person's ability to hold N not obviously related bits of data in their registers,
		and their inclinination to point out a flaw in a concluding point before all constraint
		bits have been communicated.
	</p>
	<p class="Jaap">
		The very people who urge me to: "...get to the point" are the people I struggle
		to debate with.
	</p>
	<p class="Jaap">
		I'll admit that I could possibly optimize my wordiness, but I think one great strategy
		for doing so is to defer constraints. That's only possible when two people are constructively
		reaching a goal.
	</p>
	<p class="Mike">
		(I'm lukewarm, at best, toward Eliezer's article. Unless I've missed something big,
		he's taking the *existence* of a scientific document 60 years ago, and the *existence*
		of a public relations document today, and concluding that "no one can do serious
		analysis of existential risks anymore, because just by asking the question, you're
		threatening the funding of your whole field.")
	</p>
	<p class="Jaap">
		I.e., if neither party sets out to intentionally sabotage the opponents argument,
		then we are generally more amicable to permitting minor nitpicks because we're cooperative
		in filling out the constraint space.
	</p>
	<p class="Mike">
		Agreed that there is an inverse correlation between someone's attention span, and
		their interest in refuting your point.
	</p>
	<p class="Jaap">
		How strongly Eliezier's observation about a claimed cultural observation relies
		on his sharing of the anecdotes alone is not something I think is worth spending
		a whole lot of time.
	</p>
	<p class="Jaap">
		We could analyse if the grammar of his sentences specifically stipulates that he
		derives the observation solely from anecdote, or even if not, if at least he's wording
		it such that the risk exists and that he ought to take responsibility for lowering
		that risk.
	</p>
	<p class="Mike">
		He spends most of the article relating the anecdotes, and then draws a conclusion,
		right?
	</p>
	<p class="Mike">
		I guess the thing is, he spends most of the time in the article relating the anecdotes.
	</p>
	<p class="Jaap">
		But step back for a moment and consider his general observation, that it may be
		that the public's attitude towards science today is less conducive to rational global
		risk analysis than it was during initial atom bomb development.
	</p>
	<p class="Jaap">
		Could he have done better? Yes, probably so...
	</p>
	<p class="Mike">
		As I see it, then, he could have skipped the anecdotes altogether. I just don't
		see how the existence of two documents supports his primary thesis.
	</p>
	<p class="Jaap">
		I wouldn't say they don't support the thesis, sooner only that two anecdotes are
		not sufficient to make it the most probable hypothesis.
	</p>
	<p class="Mike">
		What he seems to be overlooking is the huge volume of scientific literature which
		*does* discuss these things in much more technical terms.
	</p>
	<p class="Mike">
		That someone had to explain it to the media is simply a result of the fact that
		people have access to much more information today than 60 years ago.
	</p>
	<p class="Jaap">
		But seriously (and I'm going to sound like a between the lines ad hominem attack),
		if the intellectual rational crowd is going to spent most of its time nitpicking
		the minute any of its members rise about the crowd, we're not going to get much
		useful work done.
	</p>
	<p class="Jaap">
		Now the continuum between nitpicking and useful critique is nebulous, and perhaps
		you make a valid point here (I'll get to that). But I can't help but get an impression
		that you enter the process of reading most of Eliezier's writing specifically with
		the intent of finding flaws in it.
	</p>
	<p class="Jaap">
		Don't get me wrong, I like you better than Eliezier.
	</p>
	<p class="Jaap">
		And I look for the same flaws.
	</p>
	<p class="Jaap">
		And certainly don't want you to join me in the masturbation circle.
	</p>
	<p class="Jaap">
		But....
	</p>
	<p class="Jaap">
		I don't know. Just sharing a feeling.
	</p>
	<p class="Jaap">
		:D
	</p>
	<p class="Mike">
		Hmm, I think you may presume more animosity on my part, toward Eliezer, than actually
		exists.
	</p>
	<p class="Mike">
		I didn't actually realize, immediately, that the article was by him.
	</p>
	<p class="Jaap">
		Fair enough, point taken. Like I said, I'm sensitive to that kind of prior bias
		having seen it very much in myself and others lately.
	</p>
	<p class="Mike">
		But I *do* expect the rational crowd to do *better* than the tabloid crowd, when
		it comes to rationality.
	</p>
	<p class="Mike">
		And the main issue I have with this article is that the thesis is not at all well
		backed up by the anecdotes.
	</p>
	<p class="Mike">
		What I'd *like* to see is something along the lines of...
	</p>
	<p class="Jaap">
		Yes, and arguably Eliezier is almost suggesting (though on explicitly) that the
		alternative for the RHIC would be to release a press-release stating: "We're scientists,
		and we know what we're doing. Don't worry your puny little minds over these things.
		Move along."
	</p>
	<p class="Jaap">
		Which would've been _far_ worse than what actually happened.
	</p>
	<p class="Mike">
		Re: RHIC, there's really no alternative necessary. Clearly the press release is
		not the *only* literature on the subject.
	</p>
	<p class="Mike">
		So, there's a press release designed for one audience, and scientific literature
		designed for another.
	</p>
	<p class="Mike">
		Surely he's not suggesting that we should ignore audience when we are composing
		a document?
	</p>
	<p class="Jaap">
		Actually, let's stay on that topic for a brief second...
	</p>
	<p class="Mike">
		I assume there were press releases 60 years ago, too, but they probably contained
		less factual information than the RHIC document.
	</p>
	<p class="Jaap">
		I'm reminded of a Dawkin's strategy sometimes when I discuss vaccines.
	</p>
	<p class="Jaap">
		Dawkin's (afaik, I may confuse him with one of the other celeb atheists) *specifically*
		refuses to engage in made-to-look-like-science media debates on the topic of creationism
		or intelligent design - because he feels it would only serve to add unjustified
		scientific credibility to those ideas.
	</p>
	<p class="Jaap">
		I would imagine there exist doctors that specifically refuse to come onto Oprah
		Winfrey's show to debate with Jenny McCarthy, because it would only serve to give
		her credibility.
	</p>
	<p class="Jaap">
		Sometimes it's okay to call something for what it is; a fucking stupid idea. Until
		somebody comes with a (falsifiable) claim supported by evidence proportional to
		its extraordinarity, it's usually okay to ignore it.
	</p>
	<p class="Mike">
		Okay...
	</p>
	<p class="Mike">
		Not quite sure what the analogy is with Eliezer's article.
	</p>
	<p class="Jaap">
		What I think Eliezier is saying that the PR release was in some sense a concession
		to unreasonable fear, a permission to be irrational. Of course, a less negative
		explanation might just be that the general public preferred a slightly more comprehensible
		explanation of the risk over a blanket: "you're an idiot, the material is out there,
		go study physics, do your homework."
	</p>
	<p class="Jaap">
		The truth, as usual, is probably somewhere in the middle. I.e., Jenny McCarthy is
		a douche bag, and somewhere sometime some kids have died from adverse vaccine effects.
	</p>
	<p class="Jaap">
		Maybe.
	</p>
	<p class="Mike">
		Gotcha. So, in his mind, the existence of a press release is important not because
		it indicates the general state of things, but because it indicates that someone,
		somewhere, explained things for the public.
	</p>
	<p class="Jaap">
		And depending on the necessity of such explanation (which is subjective) this may
		either be a good thing, or a bad thing.
	</p>
	<p class="Mike">
		How about textbooks?
	</p>
	<p class="Mike">
		They tend to dumb things down a bit--things which have plenty of coverage in technical
		literature.
	</p>
	<p class="Mike">
		Obviously the author of a texbook isn't exploring new material.
	</p>
	<p class="Jaap">
		Surely you'd say that the RHIC releasing a document why the LHC is not likely to
		spawn unicorns would be a poor use of its resources?
	</p>
	<p class="Mike">
		Sure, but it seems patently obvious that it won't spawn unicorns.
	</p>
	<p class="Jaap">
		No, there's a continuum for the usefulness of various explanations of reality.
	</p>
	<p class="Jaap">
		To many scientists, it seemed patently obvious that it won't spawn earth swallowing
		black holes.
	</p>
	<p class="Mike">
		Yeah, but you'd be surprised at the low level of creativity in the general scientific
		population.
	</p>
	<p class="Mike">
		To me, it's not at all obvious. Now, it would get a lot more obvious if I looked
		up the numbers.
	</p>
	<p class="Mike">
		Eliezer talks about the RHIC not producing black holes as though it were a foregone
		conclusion.
	</p>
	<p class="Jaap">
		For somebody like Eliezier, who attaches higher credence to scientific opinion than
		to average laymen's opinion, the LHC-black-hole press release is closer to the LHC-unicorn
		press-release than for somebody like you.
	</p>
	<p class="Jaap">
		And for somebody like you it's closer than for somebody like your mom.
	</p>
	<p class="Jaap">
		And for somebody like your mom it's closer than for somebody like a neanderthaler
		who believes in unicorns.
	</p>
	<p class="Jaap">
		The above convergence does not imply correctness
	</p>
	<p class="Mike">
		I think one of the things that bothers me about the post is that he brackets a couple
		of statements with "I am not a physicist, but", but proceeds to make statements
		about what's obvious and what's not in a way that either means he has a lot more
		physics training than he lets on, or that he is just overlooking the possibility
		that these things could happen.
	</p>
	<p class="Jaap">
		I don't think he overlooks the possibility that it could happen, considering his
		overall proclivity for considering what are generally considered far-out existinsial
		risks.
	</p>
	<p class="Mike">
		How about this angle:
	</p>
	<p class="Mike">
		Consider the total productivity of the human race.
	</p>
	<p class="Mike">
		Consider the impact on that productivity if we asked each person to perform a literature
		search and determine for themselves the danger posed by the RHIC.
	</p>
	<p class="Mike">
		Even given that they had the technical knowledge to understand it, this is a formidable
		task.
	</p>
	<p class="Mike">
		Now, consider that we take a small group of people, and have them do the literature
		search, digesting their results into an article.
	</p>
	<p class="Mike">
		Way less productivity lost.
	</p>
	<p class="Mike">
		Literature searches are not easy. Looking at the RHIC article, they draw from a
		number of sources.
	</p>
	<p class="Mike">
		I don't see anything wrong with compiling that information into one place...
	</p>
	<p class="Jaap">
		But those are not necessarily the issues at stake here.
	</p>
	<p class="Jaap">
		M people are worried. P scientists have the answer.
	</p>
	<p class="Jaap">
		Worry probably has a negative effect on the economy.
	</p>
	<p class="Jaap">
		But so has the time scientists spent on dispelling the worry.
	</p>
	<p class="Jaap">
		But one could argue, the time scientists spent could bring an M individual to contribute
		to future P growth.
	</p>
	<p class="Jaap">
		And more-over that ignoring M, would probably serve mostly to make future M demands
		even bigger,.
	</p>
	<p class="Jaap">
		It's in P's best interest to sort of tag the general M along on a string.
	</p>
	<p class="Mike">
		Presumably the scientists had already compiled all of this information, though,
		or knew the precise arguments. It would take them almost no time to reproduce it
		in the form of an article.
	</p>
	<p class="Jaap">
		Striking a balance between what is worth it, and what is just plain catering to
		idiots.
	</p>
	<p class="Mike">
		Sure.
	</p>
	<p class="Jaap">
		Jup, I think you're probably right on the compiling not taking much time.
	</p>
	<p class="Jaap">
		Perhaps I should point out that I pointed out this particular article not at all
		because I thought it was particularly interesting. I only though it was coincidental
		that you mentioned the very atmosphere thing in our argument yesterday.
	</p>
	<p class="Mike">
		:) Fair enough.
	</p>
	<p class="Mike">
		It gets my goat when someone who is ostensibly rational produces arguments which
		seem patently irrational to me. Clearly there is a balance to be struck here, and
		it irritates me that someone prominent has painted it as though the balance were
		a sign of declining times.
	</p>
	<p class="Mike">
		I've seen so much bullshit posted in the guise of rationality... Just seems like
		a *truly* rational person would realize that you can't just ask the general populace
		to research on their own, or ask them just to have faith in scientists. How can
		those possibly seem like palletable possibilities to a rational person?
	</p>
	<p class="Mike">
		Seems sometimes like "rational" is used as a synonym for "academic", or perhaps
		"short-sighted".
	</p>
	<p class="Mike">
		I call it "Spock" rationality.
	</p>
	<p class="Mike">
		Because Spock, for all his ostensible rationality, was a huge blockhead some of
		the time.
	</p>
	<p class="Mike">
		(Not sure if this made it through.)
	</p>
	<p class="Mike">
		But, somehow, he managed to wear it as though it were a badge of honour. Like it
		would somehow discredit him if he could see past the basic implications of something.
	</p>
	<p class="Mike">
		Anyway...
	</p>
	<p class="Mike">
		I don't specifically have a beef with Eliezer, but I do have a sensitivity toward
		what looks like short-sighted thinking to me.
	</p>
	<p class="Jaap">
		Eliezier is indeed seemingly susceptible to a craving for pride.
	</p>
	<p class="Jaap">
		A personal request, please do not add fuel to the fire that gives "rational" a bad
		name.
	</p>
	<p class="Jaap">
		I do not recall any particular cases of Spock annoying me, though I'll admit he
		lacked a certain 3, 2, 1, cya Joie De Vivre.
	</p>
	<p class="Jaap">
		Then again, I haven't watched a whole lot of the original Star Trek.
	</p>
	<p class="Jaap">
		In regards to "rational" getting a bad-rep, see this article for an *anecdotal*
		data point.
	</p>
	<p class="Jaap">
		http://www.huffingtonpost.com/srinivasan-pillay/why-rational-thinking-is_b_183082.html
	</p>
	<p class="Mike">
		Hmm, I suppose it's the fact that he was so perplexed by anything that was "illogical",
		as though it were completely opaque to him.
	</p>
	<p class="Mike">
		But it seems clear that a sufficiently rational being should also be able to comprehend,
		to some extent, things like love.
	</p>
	<p class="Jaap">
		My view on that article; the title carries the connotation that there are problems
		with rational thinking, and then the body of the article merely states how human
		beings are not always very good at being rational. Now that latter observation certainly
		gives reason to ponder over how to cope when rationality in our brains doesn't work
		as we'
	</p>
	<p class="Jaap">
		...we'd like.
	</p>
	<p class="Jaap">
		But that says nothing to me about the actual nature of rationality (which to me
		is, so far, the only effective mechanism to reason about reality).
	</p>
	<p class="Mike">
		And, in fact, I might argue that most things in day-to-day life can be understood
		rationally.
	</p>
	<p class="Mike">
		For example, with something like love, I don't think you have to boil it down to
		an objective measure.
	</p>
	<p class="Mike">
		But I think you can postulate a human emotion called love, and strive to understand
		it using rationality.
	</p>
	<p class="Mike">
		You can even anticipate certain things about it.
	</p>
	<p class="Mike">
		Most of the "irrational" things about love are perfectly predictable, and that,
		to me, means that they should be entirely comprehensible to a rational being, even
		if they cannot be derived from a set of axioms which does not include love itself.
	</p>
	<p class="Jaap">
		"...I don't think you have to boil it down to an objective measure." I don't think
		many people argue that you'd have to, and even those who might probably don't have
		it at the top of their priority list.
	</p>
	<p class="Jaap">
		However.... (!)
	</p>
	<p class="Jaap">
		...A great many people that I respect will argue that when a person's quality of
		life suffers because of his or her inability to love or be loved to the degree that
		he or she desires, that *the* most effective mechanism for solving this problem,
		is through sound application of rational principles.
	</p>
	<p class="Jaap">
		Certainly more effective than waiting for the right month of the year when his or
		her astrology's prediction is aligned.
	</p>
	<p class="Mike">
		Totally.
	</p>
	<p class="Mike">
		Exactly what I'm thinking. The thing is, rational principles can be applied to understand
		a huge range of things.
	</p>
	<p class="Mike">
		And to solve a wide variety of problems.
	</p>
	<p class="Mike">
		The astrology example is good, actually, because astrology itself is something which,
		offhand at least, seems to be completely irrational.
	</p>
	<p class="Mike">
		That people follow astrology is not at all irrational.
	</p>
	<p class="Mike">
		Because if you know one or two things about people, I think you could foresee that
		they would follow something like that.
	</p>
	<p class="Jaap">
		But surely we shouldn't confuse the social phenomenon with the indivual's beliefs.
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		They're related somehow, but one is not a justification for theo ther.
	</p>
	<p class="Mike">
		brb.
	</p>
	<p class="Mike">
		Back.
	</p>
	<p class="Mike">
		See, this is why I like you.
	</p>
	<p class="Jaap">
		A lot of people belief X. There's an evolutionary principle that plausibly to some
		degree explains the desire to belief X. Therefore it is okay to belief X.
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		That kind of reasoning is used to justify so many apalling behavior.
	</p>
	<p class="Jaap">
		Doesn't mean I'm immune to it.
	</p>
	<p class="Mike">
		I think we can understand the desire to believe X, but it doesn't imply that X is
		a good way to understand things.
	</p>
	<p class="Mike">
		Doesn't mean it's okay to do it that way, but we can certainly foresee that someone
		might.
	</p>
	<p class="Jaap">
		But at least I try to recognize the general principle, and I'm willing to accept
		cases where I make the mistake, and then change my behaviour, or submit to my evolutionary
		weakness (lack of discipline, like say the fact that I just smoked a cigarette)
	</p>
	<p class="Jaap">
		Yes,
	</p>
	<p class="Jaap">
		You know, I'm accutely aware of how much I'm seduced by this flavor of futurism.
		I think it's for two main reasons; one is that it gives me an excuse to avoid the
		presence (i.e., in a super extreme popularism; why care about the dishes, if the
		world is going to end tomorrow), and two; that it appears to be a non-retarded minority
		view, and I like being contrary.
	</p>
	<p class="Jaap">
		The latter probably also being fuel for my BASE pursuit.
	</p>
	<p class="Jaap">
		And my purchase of Vibram Five Finger shoes.
	</p>
	<p class="Jaap">
		And so on, and so forth.
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		Now my awareness of this, makes me pursue counter-singularity points....
	</p>
	<p class="Jaap">
		So far, I'm not having much luck finding compelling cases...
	</p>
	<p class="Jaap">
		But again, perhaps that is because of my own bias, right
	</p>
	<p class="Jaap">
		The most compelling "it's not going to happen" articles are the ones that rely on
		arguments about complexity and/or unpredictability.
	</p>
	<p class="Jaap">
		Meaning; it's not going to happen anytime soon, and/or stuff may or will happen,
		but it'll almost surely not be among the cases that we thought it would be, so why
		bother.
	</p>
	<p class="Jaap">
		Any thoughts on my concern?
	</p>
	<p class="Jaap">
		You know, you can just tell me you're busy...
	</p>
	<p class="Mike">
		No, I have a few minutes at least.
	</p>
	<p class="Mike">
		Heading out with Jason and Carmen sometime, but waiting on their call.
	</p>
	<p class="Mike">
		Otherwise, just surfing Haskell stuff.
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Mike">
		Well...
	</p>
	<p class="Mike">
		My lack of concern is fueled mainly by the argument of resources.
	</p>
	<p class="Mike">
		It seems to me that real-world acquisition of resources slows things down *a lot*.
	</p>
	<p class="Mike">
		Or perhaps it's just a lack of imagination.
	</p>
	<p class="Jaap">
		Oh, wait. I've changed the topic from the particular risk of AI whiping out the
		species to the broader consequences of some singularity....
	</p>
	<p class="Jaap">
		Let'
	</p>
	<p class="Mike">
		Ah, I see.
	</p>
	<p class="Mike">
		:) :)
	</p>
	<p class="Jaap">
		Let's say, uploading, longevity, etc.
	</p>
	<p class="Mike">
		Roger.
	</p>
	<p class="Mike">
		In that case...
	</p>
	<p class="Mike">
		I think I have an interest in those things, but I tend to "float" it until it seems
		more immediately relevant.
	</p>
	<p class="Mike">
		For the moment, there are some things that are near the threshold, like cryonics.
	</p>
	<p class="Mike">
		Other things, like uploading, are interesting, but not quite on the radar for me.
	</p>
	<p class="Mike">
		AI, in the broader sense (not in the going-to-kill-us-all sense) is intriguing to
		me.
	</p>
	<p class="Mike">
		As is computation in a non-sequential manner.
	</p>
	<p class="Mike">
		These are things that *are* on my radar.
	</p>
	<p class="Jaap">
		I suppose that we're BASE jumpers and still alive lends some credence to the fact
		that we can approach minority views without being idiots.
	</p>
	<p class="Mike">
		But I think it's safe to say I have a pretty tight "relevance" filter, in some ways.
	</p>
	<p class="Mike">
		:) :)
	</p>
	<p class="Mike">
		Referring to the singularity as another minority view, I assume.
	</p>
	<p class="Jaap">
		Yes.
	</p>
	<p class="Mike">
		I have a feeling the singularity itself will also be slowed somewhat by resource
		acquisition.
	</p>
	<p class="Mike">
		Wow, reminds me of diodes, actually.
	</p>
	<p class="Jaap">
		Consider a parallel universe where suddenly overnight tomorrow, everybody would
		actually agree that it's coming, and that it's coming soon. Still permitting some
		variability, but really just variations on a theme/blend of Eliezier, Hanson, Kurtzweil,
		Goertzel, etc.
	</p>
	<p class="Mike">
		An ideal diode has a current-voltage characteristic something like I = e^V.
	</p>
	<p class="Jaap">
		Would you not imagine that things would look a little different?
	</p>
	<p class="Mike">
		A diode in series with a resistor, even a small one, is dominated by the resistance
		as V increases.
	</p>
	<p class="Mike">
		With the singularity, I feel like right now we're still in the "ideal" region. Things
		look exponential.
	</p>
	<p class="Mike">
		But I think the time it takes to do things in the real world, and to acquire resources,
		will prevent a "true" singularity--in the mathematical sense.
	</p>
	<p class="Mike">
		(Reading...)
	</p>
	<p class="Mike">
		Well, sure it would look different. But if you imagine a parallel universe populated
		by unicorns (because I know you do), that would look different too. I guess the
		question is, what of the world where everyone believes the singularity is approaching?
	</p>
	<p class="Jaap">
		True singularity in the omega point sense. Aside from the potential existential
		risk, that topic does remain firmly in the "interesting" part of my brain's bookshelf.
		Meanwhile, topics like cryonics, uploading, AI (in the less fatalistic sense), etc.
		are creeping away from that corner of my brain towards my "factors included in my
		decision making process" part of the brain.
	</p>
	<p class="Jaap">
		But unicorns are far less likely than singularities. You ask; what of that world?
	</p>
	<p class="Jaap">
		I answer...
	</p>
	<p class="Jaap">
		That world might act in a certain way that helps me evaluate better how I should
		make decisions in my world.
	</p>
	<p class="Jaap">
		If the whole world in 1935 knew for a fact that there would be a world war 2 in
		full swing by 1942 (or 39, or 40, or give or take), I think a large subset of those
		people would have made decisions differently than they actually did.
	</p>
	<p class="Jaap">
		You can say; but the world didn't know that...
	</p>
	<p class="Mike">
		Fair enough. I think the problem, from my standpoint, is that the singularity is,
		basically by definition, unknowable. And if we can't know it, then we are spending
		energy preparing for a very speculative future.
	</p>
	<p class="Jaap">
		Sure, and given that we haven't been to 2057 yet, nobody knows what'll actually
		happen. But that doesn't change the fact that probabilistic assesments/beliefs about
		possible future outcomes (be it on a five minute scale or a five decade scale) *do*
		affect our decision making; even that of the most idiotic person.
	</p>
	<p class="Mike">
		Speculating about that future, and trying to work out probabilities, is interesting,
		but not to the point that it changes my actions today.
	</p>
	<p class="Mike">
		Simply because the space of probabilities ahead of us is so broad.
	</p>
	<p class="Mike">
		Committing to one just because "something is coming" would be a bit like Pascal's
		wager.
	</p>
	<p class="Mike">
		Opportunity cost in the present.
	</p>
	<p class="Jaap">
		I guess I'm asking whether or not you agree that there is a disonance between the
		actual public's awareness on the topic, and the effect the possible significance
		of a future outcome could have on the public today.
	</p>
	<p class="Mike">
		On the five minute/five year scale, I feel like I have a handle on what will happen.
	</p>
	<p class="Mike">
		On the fifty year scale, not so much.
	</p>
	<p class="Jaap">
		True enough...
	</p>
	<p class="Mike">
		I think if we *knew* what would happen, or even had a reasonably good idea, then
		it might affect things. But even basic propositions like, "AI will eventually supercede
		us" seem slightly questionable to me.
	</p>
	<p class="Mike">
		There are so many factors that could prevent that from occuring as we see it, so
		it seems a bit pointless to prepare for one particular vision of it.
	</p>
	<p class="Mike">
		Gotta run. Back later, though, and have really been enjoying these chats.
	</p>
	<p class="Mike">
		TTYL!
	</p>
	<p class="Jaap">
		Cya!
	</p>
	<p class="Jaap">
		Ping?
	</p>
	<p class="Mike">
		Pong?
	</p>
	<p class="Jaap">
		You jumped online briefly this afternoon, but my ping came too late. I can [~only/[~primarily/jokingly-mostly]]
		assume that you're immediate offlineness was due to the fact that you saw that I
		was online and that you were not at all interested in having another time sink of
		a Jaap monologue. (...with [~a/b] being the syntax for: "I initially typed a, then
		realized that it was too strong of a statement, and therefore replaced it with b
		- yet leaving the a in (through said syntax) to indicate that I actually think about
		what I type before I submit it.")
	</p>
	<p class="Mike">
		:) :) Man, you know me well I'm fighting a cold. Back in Cranbrook again.
	</p>
	<p class="Jaap">
		Ah, right on. Once again disconnected from the reality we call the interwebs, and
		thus forced to exercise your own brain due to lack of blog stimuli. Damn, that must
		suck. Sorry to hear about your cold.
	</p>
	<p class="Mike">
		:) Your sympathy is not misplaced. I'm kicking its ass.
	</p>
	<p class="Jaap">
		Hahaha, nice. Hey, are you familiar with Dempster-Schafer Theory?
	</p>
	<p class="Mike">
		Not off-hand. Wikipediaing. Interesting. Oddly, reminds me of Kalman filters.
	</p>
	<p class="Jaap">
		It appears that Bayes Theorem (which I'm mostly motivated to read up on from OB/LW
		reading) and the Kalman filter (which I'm motivated to read up on from Wiimote tracking)
		are both subsumed (for some not entirely strict layman's definition of that word)
		into Dempster-Schafer. Of course, given how Bayes lies much closer to the root of
		the scientific method than almost any other theorem, one could argue that Bayes
		is likely to show up in a lot of places.
	</p>
	<p class="Mike">
		Oh, reading further.
	</p>
	<p class="Jaap">
		I just thought it was kinda neat how two fairly distinct interests of mine overlapped
		today.
	</p>
	<p class="Mike">
		Looks kind of like meta-probability...
	</p>
	<p class="Jaap">
		Hard to formalize meta-notions (call them computational sentiments) that have formed
		in my brain after "grokking" the Kalman filter (in particular, the two step process
		of calibration and reinforcement) appear to linger subconsciously in my readings
		on rationality and AI. Maybe it's a deceptive bias from my surfing patterns, but
		there appears to be an resurgence in converging of philosophy and math. Both in
		philosophy of thought, and philosophy of the universe. I'm sort of glad to see that,
		worried for a bit that a crystal-hippie interpretation of Godel would lead philosophers
		to drift away from math.
	</p>
	<p class="Mike">
		Did you catch the thing on Reddit the other day about "Why your friends have more
		friends than you do..."?
	</p>
	<p class="Jaap">
		No, searching... Got it.
	</p>
	<p class="Mike">
		Jason and I were chatting about that article last weekend. Lots of things fall into
		the same category. For example, the proposition that the best thing you can do in
		traffic is to change lanes. Sample bias is *very* cool.
	</p>
	<p class="Jaap">
		Hey, that's a *great* article.
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		Thanks for sharing.
	</p>
	<p class="Mike">
		No problemo. Was thinking of you when we were discussing it this weekend.
	</p>
	<p class="Jaap">
		Are you saying I'm biased? You'd better be... ;-)
	</p>
	<p class="Mike">
		LOL. Was just about to say the same regarding my statement!
	</p>
	<p class="Mike">
		Sample bias is one of these things that occupies a substantial part of my cognitive
		load, I'd say. Not like I'm always thinking about it, but it really seems to come
		up quite often.
	</p>
	<p class="Jaap">
		When you say it's part of your cognitive load, you mean mostly regarding to identifying
		it? Or to overcome it? And in other people, or in yourself, or in abstract?
	</p>
	<p class="Mike">
		At the moment, mainly identifying it. Not sure it's possible, strictly, to overcome
		it, although one certainly can learn to recognize sample bias. For now, perhaps
		the best description of my involvement is that it makes me giggle every time I see
		it.
	</p>
	<p class="Jaap">
		A particular interesting form of sampling bias comes up a lot in the Doomsday argument.
	</p>
	<p class="Mike">
		?
	</p>
	<p class="Jaap">
		You are not familiar with the Doomsday argument? Oh boy, are you in for a treat.
		Let me find a good reference... http://www.anthropic-principle.com/primer1.html
		And the particular paper that explicitly talks about sampling bias (in this case,
		"the self-indication-assumption"): http://www.anthropic-principle.com/preprints/olum/sia.pdf
	</p>
	<p class="Mike">
		Reading...
	</p>
	<p class="Jaap">
		Out of curiousity, is it sampling bias in particular that you are intruiged by,
		or other forms of bias as well?
	</p>
	<p class="Mike">
		Well, sampling bias seems to be at the bottom of many bias issues. What else do
		you have in mind?
	</p>
	<p class="Jaap">
		This paper lists some well known ones that I found fun to learn about: http://www.singinst.org/upload/cognitive-biases.pdf
		Looking at http://en.wikipedia.org/wiki/List_of_cognitive_biases, I'm not sure if
		it's obvious to say that sampling bias is at the bottom of many other bias issues
		(though I could probably be convinced that for most biases, one could define "sampling"
		such that it could be seen as a sampling bias.) That could be a fun exercise. ....
		Actually, I take my comment about the obviousness back.
	</p>
	<p class="Mike">
		That's quite a list! I'd certainly be careful about saying that sampling bias was
		at the bottom of all of those.
	</p>
	<p class="Jaap">
		If we're talking about beliefs, and likelihood or "truthiness" of a belief is determined
		through something Bayes-like, then by definition the only way to skew the result
		is to skew the input. And since the input is a data-set, it's by definition a sampling
		error. Unfortunately, for most topics of interest, the size of the input data set
		required to obtain objective degrees of validity is so large, that sampling bias
		is almost inevitable. The trick is to become efficient at avoiding the major errors.
		Treating every error as a sampling error would be impractical. It's like saying
		that every computer is a one-directional single tape Turing machine, which is true
		- but my 8-core X86 is hella faster than the Turing machine... ;) You did totally
		give me a new insight though... thank you for that! I must think about it more,
		but the notion that bias is *always* a sampling issue if you dig deep enough really
		appeals to me. It's like a Kalman filter - there's a sampling error, and you try
		to overcome it.
	</p>
	<p class="Mike">
		Yeah. I'm curious about it, actually.
	</p>
	<p class="Jaap">
		From "I'm not sure if it's obvious to say that sampling bias is at the bottom of
		many other bias issues." to the complete opposite in less than ten MSN messages.
		Nice!
	</p>
	<p class="Mike">
		:) :)
	</p>
	<p class="Jaap">
		And only just now did I notice your comment: "I'd certainly be careful about saying
		that sampling bias was at the bottom of all of those." LOL, did we just flip stances?
	</p>
	<p class="Mike">
		I think I may be back on the sample bandwagon. The first bias I picked - the base
		rate fallacy - is certainly a sampling issue. Jason and I had actually discussed
		that one in terms of false-positive AIDS tests. :) :)
	</p>
	<p class="Jaap">
		The space between "false positive AIDS tests" and ":):)" is too large for me to
		infer what you were laughing about. The sheer amount of (in-)appropriate comments
		I could make about AIDS make the whole thing rather intractable. ;)
	</p>
	<p class="Mike">
		lol. "The sheer amount of jokes I could make about AIDS make the whole thing rather
		intractable." -> There's sample bias in there somewhere, I'm sure.
	</p>
	<p class="Jaap">
		"There's sample bias in there somewhere, I'm sure." -> Are you saying I have chlamydia?
		Snap! Padamtam tjing!
	</p>
	<p class="Mike">
		LOL
	</p>
	<p class="Jaap">
		I think theoretical AI that operates on the fringe of both math and philosophy is
		very much about finding statistical representations for cognitive biases.
	</p>
	<p class="Mike">
		You know, though... For example: http://en.wikipedia.org/wiki/Denomination_effect
		A lot of cognitive biases seem more related to psychology than to statistical anomalies.
		It [~would/will] be really interesting to see what cognitive biases are exhibited
		by an AI.
	</p>
	<p class="Jaap">
		Going down the bias list some more I'm further led to believe that indeed it's all
		about sampling in the end. Look at it this way... For any finite IO port (i.e.,
		no omniscient gods), there exists an upper limit on universe size and resolution
		beyond which the IO can't avoid sampling bias. ... Man, I think I just said something
		really deep and awesome.... Though it'll probably be deep and awesome in the way
		that waking up the next day with a hangover makes deep and meaningful things from
		last night sound like moronicness. :)... I often feel there's a correlation between
		how deep something sounds, and (provided it's not crystal-worshipping ambigious
		trite) how utterly obivous it is once you see it.
	</p>
	<p class="Mike">
		Hey, interesting point.
	</p>
	<p class="Jaap">
		At any rate, imagine an AI that plays finite games of Pacman (finite meaning there's
		only N levels so you can "beat" the game - and yes, the Monte Carlo AIXI prompted
		this example...) Then as long as you can increase the IO bandwidth (from only being
		able to see one 'tile' forward and backward, to seeing the whole level and positions
		of ghosts, to knowing all possible paths that ghosts could take, and simulating
		those (sort of like Deep Blue plays chess, once you use "ghosts-don't-bang-their-heads-into-the-wall"
		heuristics)), you can reach a point where there'll be no sampling bias anymore.
		Note that not until Pacman can read the proverbial ghost's mind (e.g., have access
		to the pseudo-random-number-generator that determines their paths) is all sampling
		bias removed entirely. There's a sharp distinction between knowing what chess Grand
		Master's are *likely* to do, and actually reading Kasparov's mind. Further note
		that even in the presence of reading minds, Pacman has not yet become God. Without
		the "game-is-beatable" guarantee, it seems possible for a particular ghost pattern
		to inevitably corner Pacman. In the abscence of super-natural powers (i.e., that
		transcend the original game, like walking through walls) it *will* be game over.
		That's just a long way of saying that AI would still have to obey the laws of physics
		(the actual ones, not our current descriptions thereof.)
	</p>
	<p class="Mike">
		This feels related to my belief that even AI would be fundamentally throttled by
		its interaction with the real world. For example, it doesn't have an inside track
		on physical theory, really, so has to do experimentation just like anyone else (although
		I think we can expect its theories to be more powerful).
	</p>
	<p class="Jaap">
		Of course, I don't have to mention to you that when I say IO port, I'm obviously
		referring to your eyes and ears and skin and so forth just as much as anything.
	</p>
	<p class="Mike">
		:) :) Of course.
	</p>
	<p class="Jaap">
		I point out that I don't have to point that out, to momentarily relish in the fact
		that I actually happen to know somebody personally that I have that great privilege
		with....
	</p>
	<p class="Jaap">
		Hey, I'm gonna run to the cafetaria and get a snack, you gonna be here for a bit,
		or leaving soon?
	</p>
	<p class="Mike">
		I'll be here, I think.
	</p>
	<p class="Jaap">
		"...even AI would be fundamentally throttled by its interaction with the real world."
		I agree with that statement. Where you and I diverge I suspect, is that I think
		the much higher CTRL-Z-ability of software versus hardware versus wetware, gives
		a *vastly* superior rate of self-improvement.
	</p>
	<p class="Jaap">
		Brb. Afk.
	</p>
	<p class="Mike">
		Agree 100% on the high rate of self-improvement, I think. I've been trying to develop
		a reasonable concept of how important self-improvement is, though. I think you and
		I are inherently biased toward it, and I wonder if physical constraints might ultimately
		play a larger role than we suspect.
	</p>
	<p class="Jaap">
		Back.
	</p>
	<p class="Jaap">
		A not-even-back-of-the-napkin-level extrapolation of the physical amplification
		difference between somebody with an IQ of 70 and somebody with an IQ of 130 suggests
		to me a high likelihood of unprecedented amplication possibilities for somebody
		with an IQ of 260, let alone 1018.
	</p>
	<p class="Jaap">
		The dubious nature of what exactly IQ quantifies notwithstanding... I think it has
		some value, but once we insert lower life forms (primates, and so forth), and still
		want to plot both 70 and 130 on it, the return feels more logarithmic than exponential
		to me.
	</p>
	<p class="Jaap">
		That is, plot IQ versus amplification such that the graph fits people of 70 and
		people of 130. Now try adding both primates as well as IQs of 300.
	</p>
	<p class="Jaap">
		Hey, are you on Google Wave?
	</p>
	<p class="Mike">
		Yeah.
	</p>
	<p class="Jaap">
		Any thoughts on whether it might make a good platform for this kind of discussion?
	</p>
	<p class="Mike">
		Interesting thought. I haven't used it much, yet, because I've been lacking exactly
		that kind of opportunity.
	</p>
	<p class="Jaap">
		Well, here goes nothing...
	</p>
	<p class="Mike">
		Wanna give it a whirl?
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		I'm outta there...
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		Allright, maybe come back in a few months.
	</p>
	<p class="Mike">
		Yeah. Also, may take a bit of time to have a look at the documentation.
	</p>
	<p class="Mike">
		In the middle of a chat isn't quite the place to fumble around for some features.
	</p>
	<p class="Jaap">
		What I was really hoping for was a drawing feature.
	</p>
	<p class="Jaap">
		In particular, a simul-drawing feature.
	</p>
	<p class="Mike">
		Oh...
	</p>
	<p class="Jaap">
		Collaborative paint?
	</p>
	<p class="Mike">
		Yeah. I'd think that would be on the short list of gadgets.
	</p>
	<p class="Jaap">
		http://www.dabbleboard.com/draw?b=Guest227349&i=0&c=5c690c5f70da2ee5aa8be33e90dc4fe21f2c05d3
	</p>
	<p class="Jaap">
		Does that work?
	</p>
	<p class="Mike">
		I think so.
	</p>
	<p class="Mike">
		Jup.
	</p>
	<p class="Mike">
		I suppose what comes to mind for me is something like "locked in syndrome".
	</p>
	<p class="Mike">
		You could have all the internal amplification in the world, but if you can't interact
		with the outside world, it just drives you mad.
	</p>
	<p class="Jaap">
		You mean like: http://www.randi.org/site/index.php/swift-blog/783-this-cruel-farce-has-to-stop.html
	</p>
	<p class="Mike">
		http://en.wikipedia.org/wiki/Locked-in_syndrome
	</p>
	<p class="Jaap">
		Not to say that it doesn't exist....
	</p>
	<p class="Jaap">
		Which is one particular instance in fact of a case where higher IQ doesn't necessarily
		result in higher amplification potential.
	</p>
	<p class="Mike">
		:) :)
	</p>
	<p class="Jaap">
		But I think such anomalies do not at all diminish the interestingness of the chart.
	</p>
	<p class="Mike">
		http://en.wikipedia.org/wiki/The_Diving_Bell_and_the_Butterfly
	</p>
	<p class="Mike">
		The thing is, you can be really smart, but ultimately amplification has to amplify
		*something*.
	</p>
	<p class="Mike">
		Otherwise it's just feedback.
	</p>
	<p class="Jaap">
		Surely you agree that there exist sufficient data points to deduce some general
		correlation between "IQ" and "amplification potential"?
	</p>
	<p class="Mike">
		Sure. Off-hand, I'd say it's more or less a direct relationship.
	</p>
	<p class="Mike">
		Certainly an AI would have out-of-this world amplification potential.
	</p>
	<p class="Jaap">
		Okay, now ignore AI and anything past 130.
	</p>
	<p class="Mike">
		But what is it amplifying?
	</p>
	<p class="Jaap">
		Now draw the graph of monkeys to 70 to 130.
	</p>
	<p class="Jaap">
		Does that seem about right? I think we can argue about the straight line from 70
		to 130, but I definitely don't think there's a straight line from 0 to 70.
	</p>
	<p class="Mike">
		In fact...
	</p>
	<p class="Mike">
		I wonder if we aren't being quite strict enough about terms here.
	</p>
	<p class="Jaap">
		Allrighty then, I guess you're already far closer to my point of view.
	</p>
	<p class="Mike">
		Arguably, the whole thing will at least be exponential, but, then, what's the IQ
		of a rock?
	</p>
	<p class="Mike">
		Perhaps it's just a scalar.
	</p>
	<p class="Mike">
		What *is* "amplification potential" exactly?
	</p>
	<p class="Jaap">
		Past 130 is where things get interesting.
	</p>
	<p class="Jaap">
		And then indeed, amplication means is a strong limiter of amplification potential.
	</p>
	<p class="Mike">
		Well, thing about AI is that it can learn on a higher order than we can--i.e., it
		can recruit a bigger brain.
	</p>
	<p class="Mike">
		Which we cannot do.
	</p>
	<p class="Mike">
		*But* what data is that brain working with?
	</p>
	<p class="Jaap">
		And now actually, the locked-in-syndrome becomes interesting...
	</p>
	<p class="Jaap">
		The internet?
	</p>
	<p class="Mike">
		Perhaps, but that's the sum of human knowledge.
	</p>
	<p class="Mike">
		So, at best, the thing will know what we do, except perhaps it will be able to deduce
		some correlations we have not.
	</p>
	<p class="Jaap">
		Which, I hate to say it because it creates stupid Skynet and exploding monitor fantasies,
		becomes increasingly an (of many) engine for the world.
	</p>
	<p class="Mike">
		Not to say that's not useful, but I think it's *way* below capacity for something
		which is, essentially, arbitrarily smart.
	</p>
	<p class="Jaap">
		And a pretty bad-ass amplification mechanism.
	</p>
	<p class="Mike">
		In order to truly realize its potential, I think an AI needs to be able to explore
		the world in a new way.
	</p>
	<p class="Mike">
		But that takes time.
	</p>
	<p class="Mike">
		Not saying it's inherently too slow, but the interesting thing is that, whereas
		the AI's intelligence is essentially unbounded (i.e., in principle it could become
		as smart as resources allow), the resources are *not* unbounded.
	</p>
	<p class="Mike">
		Initially, it's stuck with the resources we've got.
	</p>
	<p class="Jaap">
		The fact that a locked-in-syndrome butterfly person in France could blink his eyes
		to chat with somebody in Brazil who'd never have to know said person is locked in....
		that's amplification!
	</p>
	<p class="Mike">
		Sure, but how *boring* would that be to a being that has unlimited intelligence?
	</p>
	<p class="Jaap">
		In a finite universe, resources tend to be bounded. But I don't think that's what
		you mean. Keeping things on a human scale, I'd say the resources are effectively
		unbounded... have you seen what virusses can do?
	</p>
	<p class="Mike">
		Think, hanging out with plumbers for ten years.
	</p>
	<p class="Jaap">
		:) lol
	</p>
	<p class="Mike">
		Ah, in terms of taking down the internet, yeah, an AI would do great there.
	</p>
	<p class="Jaap">
		Do you read Bruce Schneiers blog? Computer security is historically notoriously
		fail-prone.
	</p>
	<p class="Jaap">
		Take it down? Why on earth do that? Take it over in much more subtle ways.
	</p>
	<p class="Mike">
		Sure. But either way, do you really think an AI would be anything other than terribly
		bored by what we have on the internet, after about a week?
	</p>
	<p class="Jaap">
		I don't think it would take that much more intelligence past ultra-human to write
		a trojan that could execute some pretty nefarious code at some future point in time.
	</p>
	<p class="Jaap">
		Jup, I think you're right in that. But I'm also terribly bored by the fact that
		I have to put on new underwear every morning and can't automate that. Unfortunately,
		it's a necesary routine for me to effectively be able to reach my amplification
		potential.
	</p>
	<p class="Jaap">
		The internet is not the end, it's the means.
	</p>
	<p class="Mike">
		But it can only be the means for a limited set of things, in a way. For example,
		how would an AI conduct a novel physical experiment?
	</p>
	<p class="Jaap">
		By the way, let me state that I really enjoy that you're pushing back on the acceleration
		rate of AI. My inclination is to take a bit of a fatalistic stance on the matter,
		I think partly because it's a fun excuse to not do any work and just lean back because
		soon enough it won't matter anymore anyway. And I don't like that, so it's good
		that you push back.
	</p>
	<p class="Mike">
		(Assuming, of course, that we're placing importance on the physical world here.
		Would the AI be content not to enter the physical world?)
	</p>
	<p class="Jaap">
		(...and when I say soon, I'm still just talking: "possibly in my lifetime, but not
		even certainly.")
	</p>
	<p class="Mike">
		:) Well, I'm kind of excited by the throttling idea, because it seems to rein things
		in a bit. Otherwise it's just a bit too one-sided.
	</p>
	<p class="Mike">
		That said, of course, I'm open to the possibility I'm wrong. For example, if the
		AI is actually content to live solely in the Internet.
	</p>
	<p class="Mike">
		But, given how long I can put up with surfing the internet, I'm not sure that would
		be a fulfilling existence for an infinitely smart being.
	</p>
	<p class="Jaap">
		You don't think an AI could talk itself out?
	</p>
	<p class="Mike">
		Sure, but what would it do once it was out? The thing about the physical world is
		that things take so much more *time* than they do in the virtual one.
	</p>
	<p class="Mike">
		Just to do a basic physical experiment would take lifetimes for something that thought
		that fast.
	</p>
	<p class="Jaap">
		"Just to do a basic physical experiment would take lifetimes for something that
		thought that fast." -> I don't understand that.
	</p>
	<p class="Mike">
		Well, suppose the AI developed new theory.
	</p>
	<p class="Mike">
		Something that would advance technology beyond our current capabilities.
	</p>
	<p class="Mike">
		Unless it's content to live off our developments, it will eventually want to do
		this.
	</p>
	<p class="Mike">
		So, let's say it wants to experiment with "frame dragging" in general relativity.
	</p>
	<p class="Mike">
		Perhaps it has to launch a satellite, or build a facility.
	</p>
	<p class="Mike">
		How long did the LHC take to build?
	</p>
	<p class="Mike">
		What I'm thinking, is that these constraints, while not a really big deal for us,
		might seem *extremely* limiting to something with unlimited amplification potential.
	</p>
	<p class="Mike">
		Limiting in the sense that it's otherwise unlimited, but then suddenly it actually
		has to wait.
	</p>
	<p class="Mike">
		The physical world can only be hurried so much.
	</p>
	<p class="Jaap">
		Ah, when you say: "would take lifetimes", you mean: "would feel as several of their
		lifetimes, because they think so fast."
	</p>
	<p class="Jaap">
		An hour to AI is eternity.
	</p>
	<p class="Mike">
		Roger.
	</p>
	<p class="Jaap">
		Thank god for lightspeed eh...
	</p>
	<p class="Jaap">
		Here to save humanity once again... ;)
	</p>
	<p class="Mike">
		lol.
	</p>
	<p class="Mike">
		Just seems like basic tasks might be frustratingly slow for "someone" that smart.
	</p>
	<p class="Mike">
		That said, it could always subcontract.
	</p>
	<p class="Jaap">
		To the point where it goes: "pfff, what's the point, it's gonna take forever. Suicide..."
	</p>
	<p class="Mike">
		Speed things up a bit.
	</p>
	<p class="Mike">
		lol.
	</p>
	<p class="Mike">
		Not sure it would get that bad.
	</p>
	<p class="Mike">
		But it would certainly present a huge bottleneck.
	</p>
	<p class="Jaap">
		Boredom throws lots of people in front of trains... ;)
	</p>
	<p class="Mike">
		Perhaps the AI would throw itself in front of 4chan.
	</p>
	<p class="Jaap">
		HAHAHAHAHAHA
	</p>
	<p class="Jaap">
		BWhuawhuahuahuahua...
	</p>
	<p class="Mike">
		:) :)
	</p>
	<p class="Jaap">
		Holy fuck, that's very quotable right there.
	</p>
	<p class="Jaap">
		Nice, high five!
	</p>
	<p class="Mike">
		High five!
	</p>
	<p class="Jaap">
		Let's put aside for a moment the existential risk of AI. I'd like to hear your thoughts
		on how we might actually get there in the first place.
	</p>
	<p class="Jaap">
		Because hey, what's the point of discussing extinction if we don't even run the
		risk.
	</p>
	<p class="Jaap">
		3, 2, 1, cya! Right?
	</p>
	<p class="Jaap">
		;)
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		Holy fuck, we're so screwed... :P
	</p>
	<p class="Mike">
		How we might get where?
	</p>
	<p class="Jaap">
		Recursive self-optimization.
	</p>
	<p class="Jaap">
		I'll seed the topic...
	</p>
	<p class="Jaap">
		We'll stay in the realm of intuitive computing paradigms.
	</p>
	<p class="Mike">
		Ah, yes.
	</p>
	<p class="Jaap">
		E.g., ignoring quantum computing for a minute (which appears to be more Turing-like
		and less NP-is-my-bitch-like every day anyway)...
	</p>
	<p class="Jaap">
		So assume we have some fixed size address space...
	</p>
	<p class="Jaap">
		E.g., ignoring the one day possibility of an AI ordering or manufacturing new hardware
		for itself.
	</p>
	<p class="Mike">
		Agreed, quantum computing is a long ways out, and seems relatively specialized at
		this point.
	</p>
	<p class="Jaap">
		And we have some fixed bandwidth and latency IO ports.
	</p>
	<p class="Mike">
		:) Interesting.
	</p>
	<p class="Mike">
		I like your parameterization.
	</p>
	<p class="Jaap">
		(...hey, can I get a emoticon for "more Turing-like and less NP-is-my-bitch-like"
		please?)
	</p>
	<p class="Mike">
		:) :)
	</p>
	<p class="Jaap">
		We have a processor with some kind of instruction set.
	</p>
	<p class="Mike">
		|-)
	</p>
	<p class="Jaap">
		(or multiple processors, but really; any set of N parallel computers are just a
		slow variation of an N times faster single computer).
	</p>
	<p class="Mike">
		Well, there's something.
	</p>
	<p class="Jaap">
		Booooo, emoticon. NP is my bitch sounds totally like an awesome name for a band.
	</p>
	<p class="Mike">
		:) :)
	</p>
	<p class="Mike">
		Great band name, agreed.
	</p>
	<p class="Jaap">
		I'm interested in the fixed-address-space.
	</p>
	<p class="Jaap">
		Because we need to stick code + data in there.
	</p>
	<p class="Mike">
		In principle, it seems like we can boil this down to Turing machines.
	</p>
	<p class="Jaap">
		Of course, code = data, and data = code - blah blah blah...
	</p>
	<p class="Mike">
		In reality, I think we will see recognizable AI as a result of massive parallelism.
	</p>
	<p class="Mike">
		Because building things "wider" gives us exponential growth, whereas building them
		"faster" gives linear growth.
	</p>
	<p class="Jaap">
		As a result? You mean a practical result, not a theoretical result?
	</p>
	<p class="Mike">
		Roger.
	</p>
	<p class="Jaap">
		Ok.
	</p>
	<p class="Jaap">
		The same practicality comes into play when we talk about turing machines.
	</p>
	<p class="Jaap">
		We can execute the code on a turing machine, but you're going to need one hell of
		a long tape (large address space) and one hell of a fast tape reader/writer/mover.
	</p>
	<p class="Mike">
		Exactly.
	</p>
	<p class="Jaap">
		In other words, by climbing up on the hierarchy of instruction complexity, we lower
		the storage requirements for code, leaving more storage for data (memory, temporary
		buffers for simulation, etc.)
	</p>
	<p class="Jaap">
		But....
	</p>
	<p class="Jaap">
		But, in essence you've gained this by sacrificing code-generality through hardwiring
		it into the CPU.
	</p>
	<p class="Jaap">
		This has two major consequences...
	</p>
	<p class="Jaap">
		1. Recursive self-improvement of hardware is harder than it is for software. 2.
		Recursive self-improvement of a fixed-size bit-string is harder for more complicated
		instruction sets.
	</p>
	<p class="Mike">
		Interesting thought.
	</p>
	<p class="Jaap">
		In other words, as you climb up the instruction set complexity (and usefulness I
		might argue) hierarchy, you *lower* yourself down the ease-of-self-improvability
		hierarchy.
	</p>
	<p class="Jaap">
		I don't think I have a formal proof of this, but instinctively I think it's easier
		to analyse and evolve Turing machine programs than it is to analyse and evolve x86
		strings of the same length.
	</p>
	<p class="Jaap">
		A supporting thought for this notion is that proper analysis of x86 bit strings
		would require you to fill the address space with data about x86 definitions.
	</p>
	<p class="Jaap">
		Let's see if I can put some bogus numbers on this to give an idea....
	</p>
	<p class="Jaap">
		Address space is 1000.
	</p>
	<p class="Jaap">
		Turing machine recursively self-improving program: 200 for the current program,
		10 for storing how the host turing machine works, 790 left to work with.
	</p>
	<p class="Jaap">
		x86 machine recursively self improving program: 20 for the current program, 580
		for storing how an x86 host machine works, 400 left to work with.
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		Running speed of Turing machine, slow. Running speed of x86 machine, fast. Self-improvement
		speed of Turing machine, small changes often. Self-improvement speed of x86 machine,
		??? changes less often.
	</p>
	<p class="Jaap">
		There's lots of holes in this idea I'm sure; and the numbers are bullcrap. I'm just
		trying to illustrate a curious thought here, which is that increases in computing-efficiency
		decrease computing-introspectability.
	</p>
	<p class="Jaap">
		Hang on, I worded that poorly.
	</p>
	<p class="Jaap">
		I mean increases in complexity of models for computing.
	</p>
	<p class="Jaap">
		Sorry, I'm rambling on...
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		If you had a webcam I'd probably see you make the "blah blah blah" hand gesture
		to Elisa.
	</p>
	<p class="Mike">
		I suspect that we'll see AI more or less when it's inevitable.
	</p>
	<p class="Mike">
		Which is to say...
	</p>
	<p class="Mike">
		The factors we're talking about here would be made moot by an N-times increase in
		computational speed, for some N.
	</p>
	<p class="Mike">
		Looking 20 years in the future, following Moore, I've read that supercomputers should
		have 10,000 times the "computing power" of the brain. Forgetting for a moment that
		I have no idea how this is defined...
	</p>
	<p class="Mike">
		I think differences in definition, too, will be absorbed by a sufficient increase
		in computational power.
	</p>
	<p class="Mike">
		So, to me, 20 to 30 years seems like the timeline. I suspect we will first see AI
		simply as a result of a self-improving system run on a sufficiently powerful computer.
	</p>
	<p class="Jaap">
		That's an Ad-Vis-Moores-Brutus argument.
	</p>
	<p class="Mike">
		Maybe someone develops a system for finding faces for cameras, and it's *particularly*
		good.
	</p>
	<p class="Mike">
		"Ad-Vis-Moores-Brutus"
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Mike">
		I really think the main problem right now is a lack of horsepower.
	</p>
	<p class="Mike">
		And perhaps architecture, but that, too, is heading in the right direction.
	</p>
	<p class="Jaap">
		You don't think that ultimately (in hindsight, and possibly not understandable by
		humans) there'll sit some fairly "trivial" (meaning elegant) idea underneath recursive
		self-improvement?
	</p>
	<p class="Jaap">
		Even if not invented originally as such by humans, at least at some point reduced
		to that by the AI itself?
	</p>
	<p class="Jaap">
		Not to say that that one particular idea would be the "only" way to do intelligence,
		just that it would be one particular idea.
	</p>
	<p class="Mike">
		Sure... I think that after the initial creation of AI, we should see refinement
		of the concept.
	</p>
	<p class="Jaap">
		The same way that evolution is if you step back far enough, is actually pretty damn
		simlpe.
	</p>
	<p class="Jaap">
		And "just another way" of creating intelligence.
	</p>
	<p class="Mike">
		Right. That said, I don't think we'll ever look back and thing, "If only we had
		the right idea in 2009."
	</p>
	<p class="Jaap">
		A rather inefficient one I suspect, but arguably so far the only one that has been
		shown to work... :)
	</p>
	<p class="Mike">
		It'd be like saying that if you just had the right body, you could win a drag race
		with a 5 HP lawn mower engine.
	</p>
	<p class="Jaap">
		You don't think you could possibly contribute to this idea? Or it doesn't interest
		you enough to pursue that?
	</p>
	<p class="Mike">
		Sure, the science has been refined, but you still need a 1000 HP engine to win.
	</p>
	<p class="Jaap">
		Jup, just like the science of neurons is refined, but you still need a gazillion
		linked in an icky wet network before they're useful.
	</p>
	<p class="Mike">
		:) Exactly. And, even at that, I'm sometimes surprised how limited human potential
		is.
	</p>
	<p class="Mike">
		(And, truth be told, heartened.)
	</p>
	<p class="Jaap">
		Just like a need an atom clock on a sattelite and a large hadron collider to be
		a physicist - which isn't the case....
	</p>
	<p class="Jaap">
		I'm trying to figure out why you're less excited by this topic than I am.
	</p>
	<p class="Mike">
		:) :)
	</p>
	<p class="Mike">
		Well, I'm totally stoked by the idea, but I feel like right now our biggest limiting
		factor is horsepower, and intuitively, I think the rest is non-critical.
	</p>
	<p class="Mike">
		Which is to say...
	</p>
	<p class="Jaap">
		I must admit, AI has been a long passion of mine - pretty much the oldest and strongest
		for as long as I can remember touching computers. But I had my personal "AI winter"
		after seeing too many if-then-else forests in video-games.
	</p>
	<p class="Mike">
		Right.
	</p>
	<p class="Jaap">
		Video-games are not AI, they're magic. Having been a fairly serious hobby magician
		to the point of making money at weddings and birthday parties, I can really make
		that analogy strongly. It's all smoke and mirrors.
	</p>
	<p class="Mike">
		For me, I think, the problem is that I've done a bunch of stuff with genetic systems,
		neural networks, and so on, and have always been slightly disappointed with the
		possibilities.
	</p>
	<p class="Jaap">
		But we've agreed on that before.
	</p>
	<p class="Mike">
		Hey, good point.
	</p>
	<p class="Mike">
		Re: magic.
	</p>
	<p class="Jaap">
		I hear you on the neural network, genetics, etc. dissapointment.
	</p>
	<p class="Jaap">
		But dude, have you seen http://www.nvidia.com/object/fermi_architecture.html?
	</p>
	<p class="Jaap">
		Well, I don't have to tell you; you've had lunch with the Accelaware guys.
	</p>
	<p class="Mike">
		Okay, I admit, CUDA seems really interesting to me.
	</p>
	<p class="Mike">
		I haven't researched it enough, but was thinking the other night that it should
		be high on my list.
	</p>
	<p class="Mike">
		I am *very* excited by the present trend of massive parallelism.
	</p>
	<p class="Jaap">
		The thing is, my gut tells me that some amalgamation of "dumb" ideas let loose on
		Fermi^N is what'll thaw the AI winter.
	</p>
	<p class="Jaap">
		And the minute it happens, that almagamation will turn out to be less than 10.000
		lines of code executing on 10.000 cores.
	</p>
	<p class="Jaap">
		10.000 LOC is doable for mortals...
	</p>
	<p class="Mike">
		Agreed.
	</p>
	<p class="Jaap">
		Why not you?
	</p>
	<p class="Jaap">
		See, I have a good excuse. I don't want to destroy the world.
	</p>
	<p class="Jaap">
		;), kidding.
	</p>
	<p class="Mike">
		Well, at present, the lack of a major idea on how it might be done.
	</p>
	<p class="Mike">
		But also, lack of study in CUDA.
	</p>
	<p class="Mike">
		I have a simple idea which I think I will try implementing in the next 6 months
		(not AI, just something from work that seems like a good toy parallel project).
	</p>
	<p class="Mike">
		Wow, NVIDIA's doing a great job of putting some library support etc. behind this
		concept.
	</p>
	<p class="Jaap">
		Fair enough. I've been reading a lot on recent AI work (AIXI, OpenCog, Novamente,
		Bayesian Blah Blah, etc.) and it has invigorated my delusions of AI grandeur again.
	</p>
	<p class="Jaap">
		I know fairly certain I'm wrong, and the rate at which I skim instead of read papers
		is a good indicator thereof; but there's a big fucking carrot dangling in front
		of me and it's sooooo tempting.
	</p>
	<p class="Mike">
		Added to my bookmark bar.
	</p>
	<p class="Mike">
		Alright, time for me to go to sleep. Thanks for the ideas--will definitely be looking
		into CUDA, at least.
	</p>
	<p class="Jaap">
		Salut amigo. Thanks!
	</p>
	<p class="Jaap">
		http://prefrontal.org/files/posters/Bennett-Salmon-2009.jpg
	</p>
	<p class="Mike">
		:) I think I've seen this case before.
	</p>
	<p class="Jaap">
		"The salmon was approximately 18 inches long, weighed 3.8 lbs, and was not alive
		at the time of scanning."
	</p>
	<p class="Jaap">
		LOL, so subtlely hidden...
	</p>
	<p class="Mike">
		:) :)
	</p>
	<p class="Mike">
		I'm having a really hard time getting work done in Cranbrook. I think a familiar
		environment may be important.
	</p>
	<p class="Mike">
		:P
	</p>
	<p class="Jaap">
		Mweh, by that token I should be hyper productive this very second.
	</p>
	<p class="Jaap">
		Then again, you list familiarity as a necessary, not a sufficient requirement.
	</p>
	<p class="Mike">
		Well, not to say the converse is true.
	</p>
	<p class="Mike">
		:) Yep.
	</p>
	<p class="Mike">
		In fact, I think you've said it better than I did.
	</p>
	<p class="Mike">
		This is not a "converse" thing at all. And in any case, the only sensible thing
		would be an implication that the converse was false.
	</p>
	<p class="Mike">
		Gah.
	</p>
	<p class="Mike">
		Ah, the inverse, perhaps.
	</p>
	<p class="Jaap">
		Tricky, there's a lot in your two initial sentences that can be either reverted
		(converted) or inverted - without landing at the premise I posed.
	</p>
	<p class="Jaap">
		Alas, details.
	</p>
	<p class="Jaap">
		What are you trying to work on then, DirectAid stuff?
	</p>
	<p class="Jaap">
		And why are you in Cranbrook again, or still? Your dad's health iirc?
	</p>
	<p class="Mike">
		Elisa's dad died earlier this week. Funeral's on Tuesday.
	</p>
	<p class="Mike">
		So we're hanging out until then.
	</p>
	<p class="Mike">
		Came down just before to see him.
	</p>
	<p class="Mike">
		Previously, he'd taken ill, so we came down to see him.
	</p>
	<p class="Jaap">
		Ah, shitty. My condolences, please forward to Elisa.
	</p>
	<p class="Mike">
		Will do.
	</p>
	<p class="Jaap">
		Is that the first of one of your and Elisa's parents to pass away?
	</p>
	<p class="Mike">
		Yep. Probably the hardest, too.
	</p>
	<p class="Jaap">
		:( :( :(, really sorry mate.
	</p>
	<p class="Mike">
		Elisa was really attached to her dad.
	</p>
	<p class="Mike">
		Not that I'm not attached to my parents, but I *think* I take a slightly different
		view on their deaths. I suppose we'll see, eventually.
	</p>
	<p class="Jaap">
		I usually wouldn't bring this up in such a context [1], but I think you can handle
		this; what are your thoughts on the possibility that Elisa's dad will have just
		missed the boat in terms of longevity?
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		[1]: Meaning that I'm well aware that you don't say to a family member or friend
		at the funeral: "boy, wish he had signed up for cryonics now, eh?"
	</p>
	<p class="Mike">
		Well, I was thinking about that quite a lot. Two months ago, he was A-okay, and
		then in the last 3 weeks or so, he just got taken down by an incredibly aggressive
		lymphoma.
	</p>
	<p class="Mike">
		Was quite interesting to watch.
	</p>
	<p class="Jaap">
		Damn... how old?
	</p>
	<p class="Mike">
		70.
	</p>
	<p class="Mike">
		So, not super young, but not that old, either.
	</p>
	<p class="Mike">
		But it made me think, at some point in the future (who knows how long), this would
		be a non-issue.
	</p>
	<p class="Mike">
		It's his freakin' body that died.
	</p>
	<p class="Mike">
		And that's not really the part Elisa cared for.
	</p>
	<p class="Jaap">
		Dude, you so totally nailed it. No Alzheimer's here, that guy had a fully functional
		brain.
	</p>
	<p class="Mike">
		It really underscored for me, I think, the extent to which we lose information when
		someone dies like that.
	</p>
	<p class="Mike">
		It's exactly like nobody made a backup, and then the machine was shut down. What
		can you do?
	</p>
	<p class="Mike">
		Well, you can make backups, for one.
	</p>
	<p class="Jaap">
		I'll be curious if this event will change the strategy for you and/or Elisa, a little
		ways down the right, once the immediacy of emotional and practical (e.g., funeral,
		etc.) consequences has worn off a little.
	</p>
	<p class="Jaap">
		"...down the right" -> "...down the way"
	</p>
	<p class="Mike">
		An interesting question. For me, it doesn't quite reach the threshold of changing
		things (emotional/practical matters being more or less complete for me).
	</p>
	<p class="Mike">
		Trying to think why that is.
	</p>
	<p class="Mike">
		Ooh...
	</p>
	<p class="Mike">
		I saw something you will be interested in.
	</p>
	<p class="Mike">
		http://en.wikipedia.org/wiki/St._Petersburg_paradox
	</p>
	<p class="Mike">
		Thought of you when I read this, and here's why...
	</p>
	<p class="Jaap">
		Yeah, I've been trying to figure out the disconnect between my convincement that
		cryonics makes sense, and my paralysis towarsd actually signing up.
	</p>
	<p class="Mike">
		It's one of these "paradoxes" that isn't really a paradox at all, just a bit of
		a hiccup. But it's an interesting hiccup.
	</p>
	<p class="Jaap">
		Probably a genetic handicap towards far-mode thinking, in favor of near-mode thinking.
	</p>
	<p class="Mike">
		It's a game with infinite expected payoff, but which almost nobody would play.
	</p>
	<p class="Mike">
		Well, that is, nobody would play it at a high price, despite the infinite expected
		payoff.
	</p>
	<p class="Jaap">
		I'm familiar with the St. Petersburg paradox, yes.
	</p>
	<p class="Mike">
		Right. So...
	</p>
	<p class="Mike">
		There are a couple of theories I've read on how it is resolved, and I'm not really
		keen on either.
	</p>
	<p class="Jaap">
		I spent most of October to better my game-theory knowledge...
	</p>
	<p class="Mike">
		One is marginal utility.
	</p>
	<p class="Mike">
		Bah.
	</p>
	<p class="Jaap">
		....continue... I like where this is going.
	</p>
	<p class="Mike">
		The other is risk-aversion. But they seem to frame it oddly...
	</p>
	<p class="Mike">
		Looking for the place where I saw that one explained, but having a hard time finding
		it. I really wasn't very happy with the explanation.
	</p>
	<p class="Mike">
		In fact...
	</p>
	<p class="Mike">
		A lot of the time I'm not really happy with this kind of discussion, because it
		seems like a large number of the rebuttals are along the lines of, "Yeah, but a
		casino could never make an infinite payout," which seems to be missing the point.
	</p>
	<p class="Mike">
		Here's what I'm thinking...
	</p>
	<p class="Mike">
		Suppose buy-in was a million dollars.
	</p>
	<p class="Jaap">
		The risk-aversion explanation is that you'd run the risk of bottoming out your available
		money.
	</p>
	<p class="Mike">
		Expected payout is infinite, so we're still good. But, would you put a million on
		the line for a very small (albeit profitable) chance to win more?
	</p>
	<p class="Jaap">
		I think marginal utility is a ancestral genetic trait, related to near/far-mode
		thinking.
	</p>
	<p class="Mike">
		Ah, you know, risk aversion might be closer to what I think, then.
	</p>
	<p class="Mike">
		It may have been poorly explained where I read it.
	</p>
	<p class="Mike">
		To me, the marginal utility is not an issue.
	</p>
	<p class="Mike">
		It's not that I wouldn't play because the payoffs are a small percentage of the
		accumulated winnings.
	</p>
	<p class="Mike">
		It's simply that if I have a million dollars in my pocket, I can think of better
		ways to use it than to gamble it on winning 10 million.
	</p>
	<p class="Jaap">
		I wouldn't play because the time/expected-pay-off ratio is poor. If I spent that
		time working, I'll make more money.
	</p>
	<p class="Mike">
		Sure.
	</p>
	<p class="Mike">
		Coming around to the original point...
	</p>
	<p class="Mike">
		With regard to cryonics, I think the problem for me is that, although the expected
		payout is very high (arguably infinite), I stop short for similar reasons.
	</p>
	<p class="Mike">
		The thing is, I'm really enjoying my life right now, and in order to invest in cryonics,
		I would need to take something away from that.
	</p>
	<p class="Jaap">
		But I think that's mostly because of a genetic blindspot towards putting value on
		life extension.
	</p>
	<p class="Mike">
		I don't think so. I put huge value on it.
	</p>
	<p class="Mike">
		It is, to me, an infinite payout.
	</p>
	<p class="Jaap">
		Yeah, but you don't "feel" it./
	</p>
	<p class="Jaap">
		If you were told that you were going to die tomorrow.
	</p>
	<p class="Mike">
		It's just an infinite payout *in the future* which I have to pay into now.
	</p>
	<p class="Jaap">
		But pay some amount of money N to live another M days....
	</p>
	<p class="Jaap">
		What would N and M be...?
	</p>
	<p class="Mike">
		Hmm... An interesting question.
	</p>
	<p class="Mike">
		Another M days after when?
	</p>
	<p class="Mike">
		(I think this question itself may be more telling than the answer to it.)
	</p>
	<p class="Jaap">
		After today (I know that isn't the case for cryonics).
	</p>
	<p class="Mike">
		So, I'm going to die for sure tomorrow unless I shell out some cash.
	</p>
	<p class="Jaap">
		The short-circuit argument to that is along the: "I don't negotiate with terrorists,"
		because they're imaginging they're being mugged in a dark alley and have to pay
		100 bucks...
	</p>
	<p class="Mike">
		Are we bartering, or is the rate fixed?
	</p>
	<p class="Mike">
		If we're bartering, then I'll give you $100 a day.
	</p>
	<p class="Jaap">
		But that's not what I mean. Zoom out far enough and our bodies (/evolution) becomes
		the terrorist.
	</p>
	<p class="Mike">
		If the rate is fixed, then I'll give you my entire accumulated wealth divided by
		that rate.
	</p>
	<p class="Mike">
		Whoops.
	</p>
	<p class="Mike">
		That is, I'll give you my entire accumulated wealth.
	</p>
	<p class="Mike">
		At that rate.
	</p>
	<p class="Mike">
		If I knew I was gong to die tomorrow if I didn't spend the money, then I think it
		would be poinless to hang onto the cash, which won't do me any good when I'm wormfood.
	</p>
	<p class="Jaap">
		So _if_ it was proven that Cryonics works (which is kinda silly, because the minute
		that it gets proven, you don't have to sign up anymore - something that most people
		don't realize), you'd pay a hundred bucks a day for its effects.
	</p>
	<p class="Mike">
		That's my starting price, not my limit.
	</p>
	<p class="Mike">
		But, it's also contingent on my knowing that, otherwise, I'll die tomorrow.
	</p>
	<p class="Mike">
		If I don't know when I'm going to die, then for now, I'm inclined to play the odds.
	</p>
	<p class="Jaap">
		Yeah, the mugging argument doesn't tranlate entirely. But it forces one to momentarily
		think and realize: "wait a second, I don't want to die, I'd pay a shitload of money
		(everything I have, because when I'm dead, it's not worth anything) to live another
		day."
	</p>
	<p class="Jaap">
		Most people never get to that realization....
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		So the odds you're playing are that life-extension will come while you are still
		alive...
	</p>
	<p class="Mike">
		More or less. I see a probability of my surviving N days.
	</p>
	<p class="Jaap">
		Those are non-unrealistic odds, and they bring me to my current standing opinion
		on the future - my most simplified but most probable belief on the singularity.
	</p>
	<p class="Jaap">
		And that is....
	</p>
	<p class="Jaap">
		"I will not die of old age."
	</p>
	<p class="Mike">
		Indeed. More or less my problem, I think.
	</p>
	<p class="Jaap">
		In other words: 1. I might die in a car accident or BASE jump the next 50 years.
		2. Humanity may be whiped out. 3. Humanity may be transformed.
	</p>
	<p class="Jaap">
		4. Everything will take way longer than we think/hope, in which case.... I might
		die of old age.
	</p>
	<p class="Mike">
		Not that I think there's a high chance I will die, e.g., BASE jumping, but were
		it to happen, cryonics would be pointless.
	</p>
	<p class="Jaap">
		What's your email address to use? Gmail?
	</p>
	<p class="Mike">
		Jup.
	</p>
	<p class="Jaap">
		Sent you an email.
	</p>
	<p class="Mike">
		I certainly feel like the most likely way for me to die in the short term is major
		trauma (car accident, BASE jump, shot in the head by a rogue AI, whatever).
	</p>
	<p class="Mike">
		In the very long term, old age becomes an issue.
	</p>
	<p class="Mike">
		But that gives me lots of time to prepare.
	</p>
	<p class="Jaap">
		My email raises a point I recently thought of and one that I haven't seen before
		explicitly.
	</p>
	<p class="Mike">
		It's like the argument that I can maintain high-risk investments while I'm young,
		because convergence of my invested sum is not critical.
	</p>
	<p class="Mike">
		It's okay if it goes up/down 50%.
	</p>
	<p class="Mike">
		When I get closer to retirement, that becomes an issue.
	</p>
	<p class="Mike">
		So, my investment strategy changes.
	</p>
	<p class="Jaap">
		Wow...
	</p>
	<p class="Jaap">
		That may go for money, but not for life.
	</p>
	<p class="Mike">
		Why not?
	</p>
	<p class="Jaap">
		Or did I misinterpret?
	</p>
	<p class="Mike">
		Well, not that high-risk is the way to go for life.
	</p>
	<p class="Mike">
		But that strategy changes depending on the perceived risks.
	</p>
	<p class="Jaap">
		Because if you have enough life, you can recover from poverty. But if you're out
		of life, you can't recover.
	</p>
	<p class="Mike">
		In the case of life, it's the idea that I am unlikely to die of old age in the next
		10 years.
	</p>
	<p class="Mike">
		Or, for that matter, from lymphoma.
	</p>
	<p class="Jaap">
		That's why playing a long enough St. Petersburg isn't appealing to people, because
		they can bottom out and go bankrupt (financial equivalent of dying, you can't recover
		(within the game)).
	</p>
	<p class="Jaap">
		Ok, let me summarize.
	</p>
	<p class="Jaap">
		The statement: "...because I'm young, I can BASE jump, because I haven't invested
		a whole lot yet." That's true from an outside view (similar to how a gambler has
		an outside view onto his wallet with money)....
	</p>
	<p class="Mike">
		Ah, not quite what I meant.
	</p>
	<p class="Mike">
		(Though perhaps you are not referring to my statements.)
	</p>
	<p class="Jaap">
		You mean, "...because I'm young, the kind of death where cryonics may help is not
		likely enough compared to the other future outcomes (e.g., we invent longevity,
		AI's whipe us out, I die from BASE) that it makes it worth the money.
	</p>
	<p class="Mike">
		Or the cognitive load, really.
	</p>
	<p class="Jaap">
		And that I'm inclined to agree with... Cryonics itself is already a scientific gamble,
		and you're adding a personal gamble on top, that you'll reach death in such a way
		that it'll be amenable to cryonics.
	</p>
	<p class="Jaap">
		Yeah, agreed. The cognitive load is a bitch. Filling out forms, signing paper work,
		talking to friends/family, faxing shit.
	</p>
	<p class="Jaap">
		And I'm not even being sarcastic...
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Mike">
		I figured not.
	</p>
	<p class="Jaap">
		This guy is useful though: http://www.rudihoffman.com/cryonics_non_us.html
	</p>
	<p class="Jaap">
		But alas, I totally get your point, and I agree.
	</p>
	<p class="Jaap">
		My "wow" was to my initial (mistaken) interpretation that you translated a gambling
		analogy to life. Utility is cool and all, but I am not the dollar bill in my wallet.
		And corollary; I'm glad my dollar bill isn't conscious.
	</p>
	<p class="Mike">
		Agreed 100%.
	</p>
	<p class="Jaap">
		Did you get my email? Wondering if you have any thoughts on the "wanna be part of
		the rapture" argument.
	</p>
	<p class="Mike">
		From the page you linked: "In order for me to work with you to obtain this affordable,
		high quality coverage, however, the big requirement is this: YOU must be highly
		motivated, totally committed to following through on what is requested of you, and
		FUN and COOPERATIVE to work with."
	</p>
	<p class="Mike">
		This turns me off quite a lot. Makes it sound like the guy is doing me some kind
		of favour by offering this.
	</p>
	<p class="Jaap">
		It came up for me when I thought about AI and Friendly AI. It seems a great deal
		of people acknowledge the danger, and then run along headfirst anyway. I'm one of
		them, I can't wait to whip up some OpenCL on an NVIDIA Fermi and whipe out humanity.
	</p>
	<p class="Mike">
		I hate when people make business sound like a favour. Makes my skin crawl.
	</p>
	<p class="Jaap">
		:D
	</p>
	<p class="Mike">
		Re: email, didn't see a part about rapture. Did I miss something?
	</p>
	<p class="Jaap">
		Really? Interesting, I had the complete opposite reaction. It made me like the guy
		better.
	</p>
	<p class="Jaap">
		I think in that line of business, he works with a lot of people who underestimate
		the complexity.
	</p>
	<p class="Mike">
		But then I expect him to charge more.
	</p>
	<p class="Mike">
		Not make personal requirements.
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		His use of ALL CAPS is poor, but the content is not.
	</p>
	<p class="Mike">
		Is it possible I'm more of a capitalist than you are?
	</p>
	<p class="Mike">
		;)
	</p>
	<p class="Jaap">
		By telling me what he expects from me, he gives me the right to expect things from
		him.
	</p>
	<p class="Mike">
		Yeah, but I don't want to expect anything of him, except what's in the contract.
	</p>
	<p class="Mike">
		And that's business.
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		I've just worked in large companies longer than you, and I know that people are
		not perfect rational agents. When I talk to somebody on the phone, especially when
		it's regarding my death; I want to talk to somebody with a sense of humour and friendliness.
	</p>
	<p class="Jaap">
		By expressing that he expects the same from me, he's given me the impression that
		he operates like I do.
	</p>
	<p class="Mike">
		(All that said, the rest of what the guy says sounds fine to me, so I would be inclined
		to say he doesn't mean that last point in the way I'm inclined to take it.)
	</p>
	<p class="Jaap">
		Well, I think he gets a lot of cheap-skates. People who want to sign up once they
		hear they've got terminal cancer and only 3 months to go....
	</p>
	<p class="Jaap">
		I would imagine a healthy 29 year old is much easier to deal with...
	</p>
	<p class="Mike">
		Re: email/rapture, are you referring to the transformational event?
	</p>
	<p class="Jaap">
		Alas, I'm not defending him. Just pointing out that my reaction to that statement
		was the complete opposite than yours. And not just after you pointed it out to me
		either.
	</p>
	<p class="Jaap">
		I remember noticing the statement when I first read it. And when you quoted it,
		I expected you were going to point out that you liked it.
	</p>
	<p class="Jaap">
		Guess we're not clones after all... :D
	</p>
	<p class="Jaap">
		Ok, rapture.
	</p>
	<p class="Jaap">
		Quoting from my email...
	</p>
	<p class="Jaap">
		"Namely, that if my life has to end anyway, Id rather that happens as part of a
		big interesting transformational (if not catastrophic) event, than it happening
		as just another mundane death like the majority of deaths that have occurred before
		mine."
	</p>
	<p class="Mike">
		Roger.
	</p>
	<p class="Jaap">
		It seems more fun to die when the world ends (i.e., the end station for the species),
		than somewhere along the middle....
	</p>
	<p class="Mike">
		Hmmm...
	</p>
	<p class="Mike">
		A couple of thoughts come to mind.
	</p>
	<p class="Jaap">
		Let's say I had a button I could push to make the singularity happen, I was going
		to die in 3 months, and I knew for a fact that the singularity _will_ happen in
		3 decades. I also know that if I push the button now, 2 billion people will die,
		whereas if I don't, a delayed singularity might kill fewer people.
	</p>
	<p class="Jaap">
		What would I do?
	</p>
	<p class="Mike">
		1. I'm not completely sure I think it would be more interesting to die in a revolution
		than of old age. I may think it would be more interesting to *live* through a revolution...
		Although, even there, at the moment I'm inclined to say I'd rather have a choice
		of how I spend my time, as opposed to being forced to take up arms, etc.
	</p>
	<p class="Jaap">
		I think it'd be quite excited to push that button. Not even to save my life, but
		just to make the last 3 months more exciting.
	</p>
	<p class="Mike">
		2. I wonder what effect this perception has on my interest in, e.g., cryonics.
	</p>
	<p class="Mike">
		lol.
	</p>
	<p class="Mike">
		(Just read what you wrote.)
	</p>
	<p class="Jaap">
		Increase the viability of cryonics, and I'd be more inclined to let it sit. The
		last thing you want to do before getting frozen is piss off a lot of people.
	</p>
	<p class="Jaap">
		I think my motivation boils down to a core motivation of my life...
	</p>
	<p class="Jaap">
		Which is to figure out what the hell is going on... I.e., peek outside the universe,
		and have a look-see what put it there.
	</p>
	<p class="Jaap">
		I don't give myself much of a shot to actually accomplish that, but I like to think
		that somewhere in the universe there's a convergence of intelligence that's overcoming
		entropy towards trillion years from now Omega point.
	</p>
	<p class="Jaap">
		Maybe our species is a step along the way to that. Or maybe we're a dead-end branch...\
	</p>
	<p class="Mike">
		:) :)
	</p>
	<p class="Jaap">
		Either way, we're part of something, and I want to get the most out of it. Just
		to satisfy my own curiousity, nothing else.
	</p>
	<p class="Jaap">
		You can sort of see it as a bandwidth issue...
	</p>
	<p class="Jaap">
		The more shit happens before I die, the more stuff I can cram through my own IO
		ports, the closer I get to the final point.
	</p>
	<p class="Jaap">
		We're talking about moving a millimeter forward on a lightyear long scale, but I
		got this thing called curiousity, and it's mighty powerful.
	</p>
	<p class="Jaap">
		So fuck yeah, I'd probably push that button.
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		:D
	</p>
	<p class="Jaap">
		Unless I had signed up for cryonics, per my comment above.
	</p>
	<p class="Mike">
		I was thinking, in the shower this morning, about the notion of "leaving the planet
		for our children".
	</p>
	<p class="Mike">
		It occured to me that I don't have a really strong sense of having to leave much
		for anyone after I'm dead.
	</p>
	<p class="Jaap">
		Hell no, fuck thaqt.
	</p>
	<p class="Mike">
		I mean, if I'm good and dead, then, as far as I'm concerned, the universe is kaput.
	</p>
	<p class="Jaap">
		Amen
	</p>
	<p class="Mike">
		As you say, if cryonics becomes an option, it changes the equation substantially.
	</p>
	<p class="Jaap">
		It'll probably change once I have kids of my own, though I'm increasingly honest
		about the fact that part of the reason I'd like to have a family is because it helps
		me on my own journey....
	</p>
	<p class="Jaap">
		Bingo....
	</p>
	<p class="Mike">
		(A slightly odd thing to say, since cryonics is, to some extent, an option.)
	</p>
	<p class="Jaap">
		My kids may be more motivated to thaw me than a random stranger.
	</p>
	<p class="Jaap">
		My kids may have more closely aligned values than a random stranger.
	</p>
	<p class="Jaap">
		My kids may be more likely to go after pursuiing the same questions of the universe
		than a random stranger.
	</p>
	<p class="Mike">
		Well, and, to be honest, I think I would feel a certain irrational closeness to
		my own kids.
	</p>
	<p class="Mike">
		Completely outside the scope of objective measure.
	</p>
	<p class="Mike">
		Just that they are my genetic progeny.
	</p>
	<p class="Jaap">
		Sure family is about love, love, love, blah blah blah. I know all that. It's not
		like I'm going to train my kids as little rational soldiers forced into a scientific
		quest for the truth....
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Mike">
		lol.
	</p>
	<p class="Mike">
		Somehow love is a side-point in our discussion about kids here.
	</p>
	<p class="Jaap">
		But in some way, I *do* feel that having children derived from my own genetics and
		upbringing (home-environment) really is one of very few very effective mechanisms
		for getting help in the future...
	</p>
	<p class="Mike">
		Yeah, we're awesome.
	</p>
	<p class="Mike">
		Getting help?
	</p>
	<p class="Jaap">
		:D lol
	</p>
	<p class="Jaap">
		Yeah, I don't mean in a pension/retirement kind of way.
	</p>
	<p class="Jaap">
		I mean in a "make the world a better place" kind of way.
	</p>
	<p class="Mike">
		Yeah... I think the whole kids thing feels a bit like continuity past one's own
		death.
	</p>
	<p class="Mike">
		Not so much rationally, as emotionally.
	</p>
	<p class="Jaap">
		Mmmm, I've thought about that argument a lot, but like you say; when I die, the
		universe dies. So I'm not sure if it really holds anymore.
	</p>
	<p class="Jaap">
		Ah, right. Emotionally...
	</p>
	<p class="Jaap">
		Yeah, dying on a deathbed surrounded by your children, that does appeal to me a
		lot more than dying alone.
	</p>
	<p class="Jaap">
		But again, emotionally only. The universe is about to end in a few minutes anwyay.
	</p>
	<p class="Jaap">
		Afk, brb.
	</p>
	<p class="Mike">
		Exactly.
	</p>
	<p class="Jaap">
		If I can impart M% of my values onto my children, and the 100-M difference is not
		too deviant from a relatively moral norm, then my children will by definition help
		steer my universe towards those very values that are important to me.
	</p>
	<p class="Mike">
		Ah, although, I feel like after I'm dead, I don't really care what direction the
		universe goes in.
	</p>
	<p class="Jaap">
		Given that I'm fairly confident that my values are on a converging front-line for
		at least our species, that gives me hope.
	</p>
	<p class="Mike">
		Actually, that's what got me thinking about it this morning.
	</p>
	<p class="Jaap">
		No, but I care about my kids steering before I die.
	</p>
	<p class="Jaap">
		Especially if they steer the universe in a direction that helps me delay death.
	</p>
	<p class="Mike">
		I was considering that I might want to request *no religious presence whatsoever*
		in my funeral. But then I thought, no, I have been right before that the funeral
		is truly not for me, but for the people I leave behind. I'm gone, and so are my
		values.
	</p>
	<p class="Jaap">
		And since my very values point towards creating a universe that wants to defer death...
		the consequence is likely to be positive.
	</p>
	<p class="Jaap">
		QED.
	</p>
	<p class="Jaap">
		Afk, brb. For real now.
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Mike">
		I think, in a sense, my values continue after my death. On the one hand, I think,
		I'd like to offer the people close to me one last gift, by asking them to keep a
		mysterious all-knowing being out of the loop on this one.
	</p>
	<p class="Mike">
		On the other hand, there's no win for me there.
	</p>
	<p class="Mike">
		Whether they believe in God or not, whether they change their values after my death
		or not, makes not a bit of difference to me at that time.
	</p>
	<p class="Mike">
		Thinking that they *might* could arguably be said to make a difference to me now,
		but I'm disinclined to bank on that possibility.
	</p>
	<p class="Jaap">
		"...by asking them to keep a mysterious all-knowing being out of the loop on this
		one." I don't understand.
	</p>
	<p class="Mike">
		Well, I've been giving some thought to a will.
	</p>
	<p class="Mike">
		And, in particular, if there are any funereal requests I would like to make.
	</p>
	<p class="Mike">
		But, at the end of the day, I think the funeral is less about me, and more about
		the people who survive me, and about how they deal with that.
	</p>
	<p class="Mike">
		So, if someone wants a priest to be involved, and if that's not offensive to all
		the other surviving people. then I don't quite see how it is my decision whether
		or not it should happen.
	</p>
	<p class="Mike">
		Despite my abhorence of that particular thing.
	</p>
	<p class="Mike">
		I am, after all, dead at that point.
	</p>
	<p class="Jaap">
		Actually, that would bug me quite a bit. Obviously not once I'm dead. But moments
		before I die, I'd be hella disappointed if I knew that people in charge (who are
		hopefully people that love me) hadn't lived up to the level of rationality I aspire
		to.
	</p>
	<p class="Jaap">
		It'd be a failure of my own.
	</p>
	<p class="Mike">
		Well, I like to surround myself with fairly rational people, but I have some family
		members, for example, who I have a great affinity for, even though they are reasonably
		religious people.
	</p>
	<p class="Mike">
		I get along with them well now.
	</p>
	<p class="Mike">
		So I guess the thing is, after I die, does their not being completely rational creatures
		suddenly reflect poorly on me?
	</p>
	<p class="Mike">
		If anything, it seems like it would reflect poorly on me now.
	</p>
	<p class="Mike">
		After I'm dead, well, it could reflect poorly on my corpse, maybe, but *I've* left
		the picture completely.
	</p>
	<p class="Mike">
		(Gotta run, going to meet Elisa. Back later.)
	</p>
	<p class="Jaap">
		Cya!
	</p>
	<p class="Jaap">
		And yes, it reflects poorly on you now.
	</p>
	<p class="Jaap">
		Though only by my standards of course. Your standards may very well be different.
	</p>
	<p class="Jaap">
		Awesome: http://www.youtube.com/watch?v=-LkusicUL2s
	</p>
	<p class="Mike">
		Ah, yes.
	</p>
	<p class="Mike">
		Hey, wait, something reflects poorly on me? Sweet!
	</p>
	<p class="Jaap">
		:), I have a quick meeting. Back in 20ish.
	</p>
	<p class="Jaap">
		'Re.
	</p>
	<p class="Mike">
		OpenCL?
	</p>
	<p class="Jaap">
		What about it?
	</p>
	<p class="Mike">
		Thoughts on OpenCL vs. CUDA?
	</p>
	<p class="Jaap">
		I think the former will replace the latter long term. Afaik, even NVIDIA admits
		that.
	</p>
	<p class="Mike">
		Seems like it.
	</p>
	<p class="Mike">
		Have shifted my efforts to OpenCL.
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Mike">
		Have a problem from work in mind which is, I think, extremely parallelizable.
	</p>
	<p class="Jaap">
		Excellent. Curious to hear your thoughts on it.
	</p>
	<p class="Jaap">
		What's the problem?
	</p>
	<p class="Mike">
		Modelling reflections assuming a homogeneous subsurface. So, you have a set of perhaps
		10,000 "sources", and about the same number of "receivers" on the surface. In the
		subsurface, you have M x N bins. Now take each source-receiver pair, combined with
		each subsurface bin, and calculate the surface normal that would reflect a signal
		from the source to the receiver.
	</p>
	<p class="Mike">
		Hugely parallel. Basically, S x R x M x N parallel kernels calculating surface normals.
	</p>
	<p class="Mike">
		At least, in the naive implementation.
	</p>
	<p class="Mike">
		Could be it's better to add some sequential character in there, but looking at all
		the tutorial examples, it doesn't seem that way.
	</p>
	<p class="Mike">
		I should add, the results are binned, so you don't actuall store all the surface
		normals.
	</p>
	<p class="Mike">
		So, the output data is quite limited compared to the compuation.
	</p>
	<p class="Jaap">
		Any problem that has a 10,000 of a particular thing in it, is very likely to be
		parallizable.
	</p>
	<p class="Jaap">
		:D
	</p>
	<p class="Mike">
		Exactly.
	</p>
	<p class="Mike">
		And, looking at each reflection as a "thing", this problem likely has a billion
		to a trillion "things" that are all the same, acting on a fixed data set.
	</p>
	<p class="Mike">
		Pretty stoked to see how it performs.
	</p>
	<p class="Jaap">
		One might say it is: "embarrasingly" parallel.
	</p>
	<p class="Mike">
		:) :)
	</p>
	<p class="Jaap">
		http://en.wikipedia.org/wiki/Embarrassingly_parallel
	</p>
	<p class="Jaap">
		But you know that.
	</p>
	<p class="Mike">
		lol. Ah, yes.
	</p>
	<p class="Jaap">
		Was the service for Elisa's dad this past weekend?
	</p>
	<p class="Mike">
		No, funeral's tomorrow. There's a "prayer service" tonight.
	</p>
	<p class="Mike">
		The former I can understand; the latter, well, I can tolerate.
	</p>
	<p class="Mike">
		Happily, Elisa is no more religious than I am.
	</p>
	<p class="Mike">
		Geez, looking at the Wiki article on "embarassingly parallel", it occurs to me that
		my particular problem may more correctly be classified by some stronger statement.
	</p>
	<p class="Mike">
		I mean, their example contains perhaps a few million obvious parallel tasks.
	</p>
	<p class="Mike">
		:)
	</p>
	<p class="Jaap">
		:D
	</p>
</body>
</html>
