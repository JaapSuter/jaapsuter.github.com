---
layout: post
title: "IM Transcript"
---

# {{ page.title }}

## Part One

### Jaap
Hey, have we talked about front-loading sentences before?

### Mike
Hmm, the term seems familiar.

### Jaap
Where in talking with a person, you know said person is inclined to interrupt
you the moment they discover a flaw in your reasoning, and you thus frontload
all necessary constraints in your sentences. You can't delay constraints, because
you'll already be interrupted - put into a state where: "*I was about to say
that...*" no longer works.

The risk is, frontload too much, and the person you talk can't juggle all constraints in 
his or her short-term active memory. 

Especially when those constaints do not yet have a binding context (which would make it
easier to keep them cached) to the point you're trying to make.

That's one reason I love the written word, because then you **can** actually defer
constraints to later. You can make a point quicker, and store a body of constraints 
further down.

A side-benefit is that a reader doesn't have to listen through what you're assuming
the necessary constraints are, only reading the ones up for debate instead. This saves energy.

Regretfully, my experience shows there is a **inverse** correlation to a person's ability to
hold *N* not yet obviously related bits of data in their registers, and their inclinination to
point out a flaw in a concluding point before all constraint bits have been communicated.

The very people who urge me to: "*...get to the point*" are the people that make it difficult to
get to the point.

### Mike
Agreed that there is an inverse correlation between someone's attention span, and
their interest in refuting your point.

### Jaap
I'll admit there are times when I could possibly optimize my wordiness, but I think
one great strategy for doing so is to defer constraints. Which per my point above, is only
possible when two people are constructively reaching a goal (i.e., there is some level of
mutual trust there is overlap in values).

Put differently; if neither party sets out to intentionally sabotage the opponents argument,
we are generally more amicable to permitting minor nitpicks because we're cooperative
in filling out the constraint space.

## Part Two

### Jaap
In regards to "rational" getting a bad-rep, see this article for an **anecdotal**
data point:

 * [Why Rational Thinking Is Bad](http://www.huffingtonpost.com/srinivasan-pillay/why-rational-thinking-is_b_183082.html)

My view; the title carries the connotation that there are problems with rational
thinking, and then the body of the article merely states how human beings are not
always perfectly rational.

The latter observation is obvious to me, and studying how people wield their rational
capacity (e.g., heuristics and biases) seems like a worthwhile effort. But I don't
see how the article's title follows from that.

We're not perfectly skilled at rescueing starving children either, but few people would
title an article:

 * Why Saving Lives Of Starving Children Is Bad

But contrary to the lives of children, the debate on reason has't settled yet. We shouldn't let failures of 
our rational faculty justify anything, certainly not a claim that rationality itself is bad. So far, it's **the** 
mechanism to reason about reality, with unsurpassed effectiveness.

### Mike
And, in fact, I might argue that most things in day-to-day life can be understood
rationally.

For example, with something like love, I don't think you have to boil it down to
an objective measure. But I think you can postulate a human emotion called love, and
strive to understand it using rationality.

You can even anticipate certain things about it.

Most of the "irrational" things about love are perfectly predictable, and that,
to me, means that they should be entirely comprehensible to a rational being, even
if they cannot be derived from a set of axioms which does not include love itself.

### Jaap

I don't think many people argue that you'd have to boil it down to an objective measure. Even
those who might probably don't have it at the top of their priority list.

**However**, a great many people that I respect will argue that when a person's quality of
life suffers because of his or her inability to love or be loved to the degree that
he or she desires, that **the** most effective mechanism for solving this problem,
is through sound application of rational principles.

Certainly more effective than waiting until some newspaper's astrology prediction
aligns with the sacred crystals I hide under my pillow.

### Mike
Totally, exactly what I'm thinking. The thing is, rational principles can be applied to understand
a huge range of things.

And to solve a wide variety of problems.

The astrology example is good, actually, because astrology itself is something which,
offhand at least, seems to be completely irrational.

**However**, that people follow astrology is not at all irrational.

Because if you know one or two things about people, I think you could foresee that
they would follow something like that.

### Jaap
But surely **we shouldn't confuse the social phenomenon with the indivual's beliefs.**

They're related somehow, but one is not a justification for the other.

A lot of people believe *X*. There's an evolutionary principle that plausibly 
explains the desire to belief *X*. Therefore it is okay to belief *X*...?

That kind of reasoning is used to justify so many apalling behavior.

### Mike
See, this is why I like you.

I think we can understand the desire to believe *X*, but it doesn't imply that *X* is
a good way to understand things.

Doesn't mean it's okay to do it that way, but we can certainly foresee that someone
might.

### Jaap
Exactly, myself included; I'm certainly not immune to it.

But I make a modest effort to recognize a general principle, and I'm willing to admit
past mistakes, and then change my future behaviour, or submit to my evolutionary
weakness in all honesty (like the fact that I just drank a sugar-loaded can of soda-pop).

## Part Three

### Jaap
You know, I'm accutely aware of how much I'm seduced by this flavor of futurism. I can think of 
two reasons for this.

The first is that the singularity is a great excuse to avoid the present. Why care about the dishes, if the world is going to end tomorrow?

Secondly, my record shows I have built-in desire to look for (what I at a particular point consider) the rational minority. It's not always
clear how much of that is just to be contrary for contrary's sake, and how much is a genuine and sincere desire to learn. 

### Mike
    
Ack... :-)

### Jaap
Now my awareness of this, makes me pursue counter-singularity points. So far, I'm not having much
luck finding compelling cases. But again, perhaps that is because of my own bias, right

The most compelling "*it's not going to happen*" articles are the ones that rely on
arguments about complexity and/or unpredictability.

Meaning; it's not going to happen anytime soon, and/or stuff may or will happen,
but it'll almost surely not be among the cases that we thought it would be, so why
bother.

Any thoughts on my concern? Let me know if you're busy...

### Mike
No, I have a few minutes at least. Heading out with Jason and
Carmen sometime, but waiting on their call. Otherwise, just surfing
Haskell stuff.

### Jaap
I suppose that we're BASE jumpers and still alive lends some credence to the fact
that we can approach minority views without being idiots.

### Mike
Nice... :)

Referring to the singularity as another minority view, I assume.

### Jaap
Yes.

### Mike
Well... my lack of concern is fueled mainly by the argument of resources.

It seems to me that real-world acquisition of resources slows things down *a lot*.

Or perhaps it's just a lack of imagination.

### Jaap
Oh, wait - you're talking about **FOOM**. I should clarify; I've broadened the
topic from the specific risk of AI whiping out the species to any and all consequences
of some kind of singularity (such as nanotech, robotics, uploading, longevity, etc.)

### Mike
Ah, I see...     :) :)

Roger.

In that case...

I think I have an interest in those things, but I tend to "*float*" it until it seems
more immediately relevant.

For the moment, there are some things that are near the threshold, like cryonics.

Other things, like uploading, are interesting, but not quite on the radar for me.

AI, in the broader sense (not in the going-to-kill-us-all sense) is intriguing to
me. As is computation in a non-sequential manner.

These are things that *are* on my radar.

But I think it's safe to say I have a pretty tight "*relevance*" filter, in some ways.

### Mike
I have a feeling the singularity itself will also be slowed somewhat by resource
acquisition.

Wow, reminds me of diodes, actually.

An ideal diode has a current-voltage characteristic something like *I = e^V*. A diode in series with a
resistor, even a small one, is dominated by the resistance as *V* increases.

With the singularity, I feel like right now we're still in the "ideal" region. Things
look exponential.

But I think the time it takes to do things in the real world, and to acquire resources,
will prevent a "true" singularity - in the mathematical sense.

### Jaap
Interesting analogy - though I must admit my Maxwell's equations are a little rusty
to say if I agree. The domination point seems valid, but really we're just talking
laws of physics. Like a resistor, an AI would have to obey as well.

I wouldn't completely discount the possibility of AI going FTL, but if the speed of light
turns out to be a critical ingredient for Friendly AI; then I don't think it'll be the
breaking point in the end - too many other problems closer to home.

What I'm really trying to get your thoughts on, is more a public discourse kind of thing. I
have a sense there's a dissonance between my interest in the topic, its weight, and how
skeptical and/or indifferent the majority view is.

Consider a parallel universe where suddenly overnight tomorrow, everybody would
actually agree that it's coming, and that it's coming soon. Still permitting some
unpredictability, but really just variations on a theme/blend of Kurtzweil, Eliezier, Hanson,
Bostrom, Goertzel, De Grey, et al.

Would you not imagine that things would look a little different?

### Mike
(Reading...)

Well, sure it would look different. But if you imagine a parallel universe populated
by unicorns (because I know you do), that would look different too. I guess the
question is, what of the world where everyone believes the singularity is approaching?

### Jaap
True singularity in the omega point sense. Aside from the potential existential
risk, that topic does remain firmly in the "*interesting*" part of my brain's bookshelf.
Meanwhile, topics like cryonics, uploading, AI (in the less fatalistic sense), etc.
are creeping away from that corner of my brain towards my "*factors included in my
near-mode decision making process*" part of the brain.

 * **Unicorns are far less likely than singularities.**

You ask; what of that world? I answer...

That world might behave in a way that helps me evaluate better how I should
make decisions in my world.

If the whole world in 1935 knew for a fact that there would be another world war in
full swing by 1942 (or 39, or 40, depending), I think a large subset of those
people would have made a few decisions differently than they actually did.

You might say; but the world didn't know that...

### Mike
Fair enough. I think the problem, from my standpoint, is that the singularity is,
basically by definition, unknowable. And if we can't know it, then we are spending
energy preparing for a very speculative future.

### Jaap
Sure, and given that we haven't been to 2057 yet, nobody knows what'll actually
happen. But that doesn't change the fact that probabilistic assesments/beliefs about
possible future outcomes (be it on a five minute scale or a five decade scale) *do*
affect our decision making; even that of the most idiotic person.

### Mike
Speculating about that future, and trying to work out probabilities, is interesting,
but not to the point that it changes my actions today.

Simply because the space of probabilities ahead of us is so broad.

Committing to one just because "something is coming" would be a bit like Pascal's
wager.

There's opportunity cost in the present.

### Jaap
I guess I'm asking whether or not you agree that there is a dissonance between the
actual public's awareness on the topic, and the effect the possible significance
of a future outcome could have on the public today.

### Mike
On the five minute/five year scale, I feel like I have a handle on what will happen.

On the fifty year scale, not so much.

### Jaap
True enough...

### Mike

I think if we *knew* what would happen, or even had a reasonably good idea, then
it might affect things. But even basic propositions like, "*AI will eventually supercede
us*" seem slightly questionable to me.

There are so many factors that could prevent that from occuring as we see it, so
it seems a bit pointless to prepare for one particular vision of it.

Gotta run. Back later, though, and have really been enjoying these chats.

TTYL!

### Jaap
Cya!

## Part Four

### Jaap
Ping?

### Mike
Pong?

### Jaap
You jumped online briefly this afternoon, but my ping came too late. I can [~only/[~primarily/jokingly-mostly]]
assume that you're immediate offlineness was due to the fact that you saw that I
was online and that you were not at all interested in having another time sink of
a Jaap monologue.

(...with [~a/b] being the syntax for: "I initially typed a, then
realized that it was too strong of a statement, and therefore replaced it with b
- yet leaving the a in (through said syntax) to indicate that I actually think about
what I type before I submit it.")

### Mike
Man, you know me well... :) :) 

I'm fighting a cold. Back in Cranbrook again.

### Jaap
Ah, right on. Once again disconnected from the reality we call the interwebs, and
thus forced to exercise your own brain due to lack of blog stimuli. Damn, that must
suck. Sorry to hear about your cold.

### Mike
Your sympathy is not misplaced. I'm kicking its ass...    :-)

### Jaap
Hahaha, nice. Hey, are you familiar with Dempster-Schafer Theory?

### Mike
Not off-hand. Wikipediaing. Interesting. Oddly, reminds me of Kalman filters.

### Jaap
It appears that Bayes Theorem (which I'm mostly motivated to read up on from OB/LW
reading) and the Kalman filter (which I'm motivated to read up on from Wiimote tracking)
both come up alongside Dempster-Schafer.

Of course, given how Bayes lies much closer to the root of the scientific method than
almost any other theorem, one could argue that Bayes is likely to show up in a lot of places.

### Mike
Oh, reading further.

### Jaap
I just thought it was kinda neat how two fairly distinct interests of mine overlapped
today.

### Mike
Looks kind of like meta-probability...

### Jaap
Hard to formalize meta-notions (call them computational sentiments) that have formed
in my brain after "grokking" the Kalman filter (in particular, the two step process
of calibration and reinforcement) appear to linger subconsciously in my readings
on rationality and AI. Maybe it's a deceptive bias from my surfing patterns, but
there appears to be an resurgence in converging of philosophy and math. Both in
philosophy of thought, and philosophy of the universe. I'm sort of glad to see that,
worried for a bit that a crystal-hippie interpretation of Godel would lead philosophers
to drift away from math.

### Mike
Did you catch the thing on Reddit the other day about "Why your friends have more
friends than you do..."?

### Jaap
No, searching... Got it.

### Mike
Jason and I were chatting about that article last weekend. Lots of things fall into
the same category. For example, the proposition that the best thing you can do in
traffic is to change lanes. Sample bias is *very* cool.

### Jaap
Hey, that's a *great* article.

### Mike
:)

### Jaap
Thanks for sharing.

### Mike
No problemo. Was thinking of you when we were discussing it this weekend.

### Jaap
Are you saying I'm biased? You'd better be... ;-)

### Mike
LOL. Was just about to say the same regarding my statement!

### Mike
Sample bias is one of these things that occupies a substantial part of my cognitive
load, I'd say. Not like I'm always thinking about it, but it really seems to come
up quite often.

### Jaap
When you say it's part of your cognitive load, you mean mostly regarding to identifying
it? Or to overcome it? And in other people, or in yourself, or in abstract?

### Mike
At the moment, mainly identifying it. Not sure it's possible, strictly, to overcome
it, although one certainly can learn to recognize sample bias. For now, perhaps
the best description of my involvement is that it makes me giggle every time I see
it.

### Jaap
A particular interesting form of sampling bias comes up a lot in the Doomsday argument.

### Mike
?

### Jaap
You are not familiar with the Doomsday argument? Oh boy, are you in for a treat.
Let me find a good reference... http://www.anthropic-principle.com/primer1.html
And the particular paper that explicitly talks about sampling bias (in this case,
"the self-indication-assumption"): http://www.anthropic-principle.com/preprints/olum/sia.pdf

### Mike
Reading...

### Jaap
Out of curiousity, is it sampling bias in particular that you are intruiged by,
or other forms of bias as well?

### Mike
Well, sampling bias seems to be at the bottom of many bias issues. What else do
you have in mind?

### Jaap
This paper lists some well known ones that I found fun to learn about: http://www.singinst.org/upload/cognitive-biases.pdf
Looking at http://en.wikipedia.org/wiki/List_of_cognitive_biases, I'm not sure if
it's obvious to say that sampling bias is at the bottom of many other bias issues
(though I could probably be convinced that for most biases, one could define "sampling"
such that it could be seen as a sampling bias.) That could be a fun exercise. ....
Actually, I take my comment about the obviousness back.

### Mike
That's quite a list! I'd certainly be careful about saying that sampling bias was
at the bottom of all of those.

### Jaap
If we're talking about beliefs, and likelihood or "truthiness" of a belief is determined
through something Bayes-like, then by definition the only way to skew the result
is to skew the input. And since the input is a data-set, it's by definition a sampling
error. Unfortunately, for most topics of interest, the size of the input data set
required to obtain objective degrees of validity is so large, that sampling bias
is almost inevitable. The trick is to become efficient at avoiding the major errors.
Treating every error as a sampling error would be impractical. It's like saying
that every computer is a one-directional single tape Turing machine, which is true
- but my 8-core X86 is hella faster than the Turing machine... ;) You did totally
give me a new insight though... thank you for that! I must think about it more,
but the notion that bias is *always* a sampling issue if you dig deep enough really
appeals to me. It's like a Kalman filter - there's a sampling error, and you try
to overcome it.

### Mike
Yeah. I'm curious about it, actually.

### Jaap
From "I'm not sure if it's obvious to say that sampling bias is at the bottom of
many other bias issues." to the complete opposite in less than ten MSN messages.
Nice!

### Mike
:) :)

### Jaap
And only just now did I notice your comment: "I'd certainly be careful about saying
that sampling bias was at the bottom of all of those." LOL, did we just flip stances?

### Mike
I think I may be back on the sample bandwagon. The first bias I picked - the base
rate fallacy - is certainly a sampling issue. Jason and I had actually discussed
that one in terms of false-positive AIDS tests. :) :)

### Jaap
The space between "false positive AIDS tests" and ":):)" is too large for me to
infer what you were laughing about. The sheer amount of (in-)appropriate comments
I could make about AIDS make the whole thing rather intractable. ;)

### Mike
lol. "The sheer amount of jokes I could make about AIDS make the whole thing rather
intractable." -> There's sample bias in there somewhere, I'm sure.

### Jaap
"There's sample bias in there somewhere, I'm sure." -> Are you saying I have chlamydia?
Snap! Padamtam tjing!

### Mike
LOL

### Jaap
I think theoretical AI that operates on the fringe of both math and philosophy is
very much about finding statistical representations for cognitive biases.

### Mike
You know, though... For example: http://en.wikipedia.org/wiki/Denomination_effect
A lot of cognitive biases seem more related to psychology than to statistical anomalies.
It [~would/will] be really interesting to see what cognitive biases are exhibited
by an AI.

### Jaap
Going down the bias list some more I'm further led to believe that indeed it's all
about sampling in the end. Look at it this way... For any finite IO port (i.e.,
no omniscient gods), there exists an upper limit on universe size and resolution
beyond which the IO can't avoid sampling bias. ... Man, I think I just said something
really deep and awesome.... Though it'll probably be deep and awesome in the way
that waking up the next day with a hangover makes deep and meaningful things from
last night sound like moronicness. :)... I often feel there's a correlation between
how deep something sounds, and (provided it's not crystal-worshipping ambigious
trite) how utterly obivous it is once you see it.

### Mike
Hey, interesting point.

### Jaap
At any rate, imagine an AI that plays finite games of Pacman (finite meaning there's
only N levels so you can "beat" the game - and yes, the Monte Carlo AIXI prompted
this example...) Then as long as you can increase the IO bandwidth (from only being
able to see one 'tile' forward and backward, to seeing the whole level and positions
of ghosts, to knowing all possible paths that ghosts could take, and simulating
those (sort of like Deep Blue plays chess, once you use "ghosts-don't-bang-their-heads-into-the-wall"
heuristics)), you can reach a point where there'll be no sampling bias anymore.
Note that not until Pacman can read the proverbial ghost's mind (e.g., have access
to the pseudo-random-number-generator that determines their paths) is all sampling
bias removed entirely. There's a sharp distinction between knowing what chess Grand
Master's are *likely* to do, and actually reading Kasparov's mind. Further note
that even in the presence of reading minds, Pacman has not yet become God. Without
the "game-is-beatable" guarantee, it seems possible for a particular ghost pattern
to inevitably corner Pacman. In the abscence of super-natural powers (i.e., that
transcend the original game, like walking through walls) it *will* be game over.
That's just a long way of saying that AI would still have to obey the laws of physics
(the actual ones, not our current descriptions thereof.)

### Mike
This feels related to my belief that even AI would be fundamentally throttled by
its interaction with the real world. For example, it doesn't have an inside track
on physical theory, really, so has to do experimentation just like anyone else (although
I think we can expect its theories to be more powerful).

### Jaap
Of course, I don't have to mention to you that when I say IO port, I'm obviously
referring to your eyes and ears and skin and so forth just as much as anything.

### Mike
:) :) Of course.

### Jaap
I point out that I don't have to point that out, to momentarily relish in the fact
that I actually happen to know somebody personally that I have that great privilege
with....

### Jaap
Hey, I'm gonna run to the cafetaria and get a snack, you gonna be here for a bit,
or leaving soon?

### Mike
I'll be here, I think.

### Jaap
"...even AI would be fundamentally throttled by its interaction with the real world."
I agree with that statement. Where you and I diverge I suspect, is that I think
the much higher CTRL-Z-ability of software versus hardware versus wetware, gives
a *vastly* superior rate of self-improvement.

### Jaap
Brb. Afk.

### Mike
Agree 100% on the high rate of self-improvement, I think. I've been trying to develop
a reasonable concept of how important self-improvement is, though. I think you and
I are inherently biased toward it, and I wonder if physical constraints might ultimately
play a larger role than we suspect.

### Jaap
Back.

### Jaap
A not-even-back-of-the-napkin-level extrapolation of the physical amplification
difference between somebody with an IQ of 70 and somebody with an IQ of 130 suggests
to me a high likelihood of unprecedented amplication possibilities for somebody
with an IQ of 260, let alone 1018.

### Jaap
The dubious nature of what exactly IQ quantifies notwithstanding... I think it has
some value, but once we insert lower life forms (primates, and so forth), and still
want to plot both 70 and 130 on it, the return feels more logarithmic than exponential
to me.

### Jaap
That is, plot IQ versus amplification such that the graph fits people of 70 and
people of 130. Now try adding both primates as well as IQs of 300.

### Jaap
Hey, are you on Google Wave?

### Mike
Yeah.

### Jaap
Any thoughts on whether it might make a good platform for this kind of discussion?

### Mike
Interesting thought. I haven't used it much, yet, because I've been lacking exactly
that kind of opportunity.

### Jaap
Well, here goes nothing...

### Mike
Wanna give it a whirl?

### Mike
:)

### Jaap
I'm outta there...

### Mike
:)

### Jaap
Allright, maybe come back in a few months.

### Mike
Yeah. Also, may take a bit of time to have a look at the documentation.

### Mike
In the middle of a chat isn't quite the place to fumble around for some features.

### Jaap
What I was really hoping for was a drawing feature.

### Jaap
In particular, a simul-drawing feature.

### Mike
Oh...

### Jaap
Collaborative paint?

### Mike
Yeah. I'd think that would be on the short list of gadgets.

### Jaap
http://www.dabbleboard.com/draw?b=Guest227349&i=0&c=5c690c5f70da2ee5aa8be33e90dc4fe21f2c05d3

### Jaap
Does that work?

### Mike
I think so.

### Mike
Jup.

### Mike
I suppose what comes to mind for me is something like "locked in syndrome".

### Mike
You could have all the internal amplification in the world, but if you can't interact
with the outside world, it just drives you mad.

### Jaap
You mean like: http://www.randi.org/site/index.php/swift-blog/783-this-cruel-farce-has-to-stop.html

### Mike
http://en.wikipedia.org/wiki/Locked-in_syndrome

### Jaap
Not to say that it doesn't exist....

### Jaap
Which is one particular instance in fact of a case where higher IQ doesn't necessarily
result in higher amplification potential.

### Mike
:) :)

### Jaap
But I think such anomalies do not at all diminish the interestingness of the chart.

### Mike
http://en.wikipedia.org/wiki/The_Diving_Bell_and_the_Butterfly

### Mike
The thing is, you can be really smart, but ultimately amplification has to amplify
*something*.

### Mike
Otherwise it's just feedback.

### Jaap
Surely you agree that there exist sufficient data points to deduce some general
correlation between "IQ" and "amplification potential"?

### Mike
Sure. Off-hand, I'd say it's more or less a direct relationship.

### Mike
Certainly an AI would have out-of-this world amplification potential.

### Jaap
Okay, now ignore AI and anything past 130.

### Mike
But what is it amplifying?

### Jaap
Now draw the graph of monkeys to 70 to 130.

### Jaap
Does that seem about right? I think we can argue about the straight line from 70
to 130, but I definitely don't think there's a straight line from 0 to 70.

### Mike
In fact...

### Mike
I wonder if we aren't being quite strict enough about terms here.

### Jaap
Allrighty then, I guess you're already far closer to my point of view.

### Mike
Arguably, the whole thing will at least be exponential, but, then, what's the IQ
of a rock?

### Mike
Perhaps it's just a scalar.

### Mike
What *is* "amplification potential" exactly?

### Jaap
Past 130 is where things get interesting.

### Jaap
And then indeed, amplication means is a strong limiter of amplification potential.

### Mike
Well, thing about AI is that it can learn on a higher order than we can--i.e., it
can recruit a bigger brain.

### Mike
Which we cannot do.

### Mike
*But* what data is that brain working with?

### Jaap
And now actually, the locked-in-syndrome becomes interesting...

### Jaap
The internet?

### Mike
Perhaps, but that's the sum of human knowledge.

### Mike
So, at best, the thing will know what we do, except perhaps it will be able to deduce
some correlations we have not.

### Jaap
Which, I hate to say it because it creates stupid Skynet and exploding monitor fantasies,
becomes increasingly an (of many) engine for the world.

### Mike
Not to say that's not useful, but I think it's *way* below capacity for something
which is, essentially, arbitrarily smart.

### Jaap
And a pretty bad-ass amplification mechanism.

### Mike
In order to truly realize its potential, I think an AI needs to be able to explore
the world in a new way.

### Mike
But that takes time.

### Mike
Not saying it's inherently too slow, but the interesting thing is that, whereas
the AI's intelligence is essentially unbounded (i.e., in principle it could become
as smart as resources allow), the resources are *not* unbounded.

### Mike
Initially, it's stuck with the resources we've got.

### Jaap
The fact that a locked-in-syndrome butterfly person in France could blink his eyes
to chat with somebody in Brazil who'd never have to know said person is locked in....
that's amplification!

### Mike
Sure, but how *boring* would that be to a being that has unlimited intelligence?

### Jaap
In a finite universe, resources tend to be bounded. But I don't think that's what
you mean. Keeping things on a human scale, I'd say the resources are effectively
unbounded... have you seen what virusses can do?

### Mike
Think, hanging out with plumbers for ten years.

### Jaap
:) lol

### Mike
Ah, in terms of taking down the internet, yeah, an AI would do great there.

### Jaap
Do you read Bruce Schneiers blog? Computer security is historically notoriously
fail-prone.

### Jaap
Take it down? Why on earth do that? Take it over in much more subtle ways.

### Mike
Sure. But either way, do you really think an AI would be anything other than terribly
bored by what we have on the internet, after about a week?

### Jaap
I don't think it would take that much more intelligence past ultra-human to write
a trojan that could execute some pretty nefarious code at some future point in time.

### Jaap
Jup, I think you're right in that. But I'm also terribly bored by the fact that
I have to put on new underwear every morning and can't automate that. Unfortunately,
it's a necesary routine for me to effectively be able to reach my amplification
potential.

### Jaap
The internet is not the end, it's the means.

### Mike
But it can only be the means for a limited set of things, in a way. For example,
how would an AI conduct a novel physical experiment?

### Jaap
By the way, let me state that I really enjoy that you're pushing back on the acceleration
rate of AI. My inclination is to take a bit of a fatalistic stance on the matter,
I think partly because it's a fun excuse to not do any work and just lean back because
soon enough it won't matter anymore anyway. And I don't like that, so it's good
that you push back.

### Mike
(Assuming, of course, that we're placing importance on the physical world here.
Would the AI be content not to enter the physical world?)

### Jaap
(...and when I say soon, I'm still just talking: "possibly in my lifetime, but not
even certainly.")

### Mike
:) Well, I'm kind of excited by the throttling idea, because it seems to rein things
in a bit. Otherwise it's just a bit too one-sided.

### Mike
That said, of course, I'm open to the possibility I'm wrong. For example, if the
AI is actually content to live solely in the Internet.

### Mike
But, given how long I can put up with surfing the internet, I'm not sure that would
be a fulfilling existence for an infinitely smart being.

### Jaap
You don't think an AI could talk itself out?

### Mike
Sure, but what would it do once it was out? The thing about the physical world is
that things take so much more *time* than they do in the virtual one.

### Mike
Just to do a basic physical experiment would take lifetimes for something that thought
that fast.

### Jaap
"Just to do a basic physical experiment would take lifetimes for something that
thought that fast." -> I don't understand that.

### Mike
Well, suppose the AI developed new theory.

### Mike
Something that would advance technology beyond our current capabilities.

### Mike
Unless it's content to live off our developments, it will eventually want to do
this.

### Mike
So, let's say it wants to experiment with "frame dragging" in general relativity.

### Mike
Perhaps it has to launch a satellite, or build a facility.

### Mike
How long did the LHC take to build?

### Mike
What I'm thinking, is that these constraints, while not a really big deal for us,
might seem *extremely* limiting to something with unlimited amplification potential.

### Mike
Limiting in the sense that it's otherwise unlimited, but then suddenly it actually
has to wait.

### Mike
The physical world can only be hurried so much.

### Jaap
Ah, when you say: "would take lifetimes", you mean: "would feel as several of their
lifetimes, because they think so fast."

### Jaap
An hour to AI is eternity.

### Mike
Roger.

### Jaap
Thank god for lightspeed eh...

### Jaap
Here to save humanity once again... ;)

### Mike
lol.

### Mike
Just seems like basic tasks might be frustratingly slow for "someone" that smart.

### Mike
That said, it could always subcontract.

### Jaap
To the point where it goes: "pfff, what's the point, it's gonna take forever. Suicide..."

### Mike
Speed things up a bit.

### Mike
lol.

### Mike
Not sure it would get that bad.

### Mike
But it would certainly present a huge bottleneck.

### Jaap
Boredom throws lots of people in front of trains... ;)

### Mike
Perhaps the AI would throw itself in front of 4chan.

### Jaap
HAHAHAHAHAHA

### Jaap
BWhuawhuahuahuahua...

### Mike
:) :)

### Jaap
Holy fuck, that's very quotable right there.

### Jaap
Nice, high five!

### Mike
High five!

### Jaap
Let's put aside for a moment the existential risk of AI. I'd like to hear your thoughts
on how we might actually get there in the first place.

### Jaap
Because hey, what's the point of discussing extinction if we don't even run the
risk.

### Jaap
3, 2, 1, cya! Right?

### Jaap
;)

### Mike
:)

### Jaap
Holy fuck, we're so screwed... :P

### Mike
How we might get where?

### Jaap
Recursive self-optimization.

### Jaap
I'll seed the topic...

### Jaap
We'll stay in the realm of intuitive computing paradigms.

### Mike
Ah, yes.

### Jaap
E.g., ignoring quantum computing for a minute (which appears to be more Turing-like
and less NP-is-my-bitch-like every day anyway)...

### Jaap
So assume we have some fixed size address space...

### Jaap
E.g., ignoring the one day possibility of an AI ordering or manufacturing new hardware
for itself.

### Mike
Agreed, quantum computing is a long ways out, and seems relatively specialized at
this point.

### Jaap
And we have some fixed bandwidth and latency IO ports.

### Mike
:) Interesting.

### Mike
I like your parameterization.

### Jaap
(...hey, can I get a emoticon for "more Turing-like and less NP-is-my-bitch-like"
please?)

### Mike
:) :)

### Jaap
We have a processor with some kind of instruction set.

### Mike
|-)

### Jaap
(or multiple processors, but really; any set of N parallel computers are just a
slow variation of an N times faster single computer).

### Mike
Well, there's something.

### Jaap
Booooo, emoticon. NP is my bitch sounds totally like an awesome name for a band.

### Mike
:) :)

### Mike
Great band name, agreed.

### Jaap
I'm interested in the fixed-address-space.

### Jaap
Because we need to stick code + data in there.

### Mike
In principle, it seems like we can boil this down to Turing machines.

### Jaap
Of course, code = data, and data = code - blah blah blah...

### Mike
In reality, I think we will see recognizable AI as a result of massive parallelism.

### Mike
Because building things "wider" gives us exponential growth, whereas building them
"faster" gives linear growth.

### Jaap
As a result? You mean a practical result, not a theoretical result?

### Mike
Roger.

### Jaap
Ok.

### Jaap
The same practicality comes into play when we talk about turing machines.

### Jaap
We can execute the code on a turing machine, but you're going to need one hell of
a long tape (large address space) and one hell of a fast tape reader/writer/mover.

### Mike
Exactly.

### Jaap
In other words, by climbing up on the hierarchy of instruction complexity, we lower
the storage requirements for code, leaving more storage for data (memory, temporary
buffers for simulation, etc.)

### Jaap
But....

### Jaap
But, in essence you've gained this by sacrificing code-generality through hardwiring
it into the CPU.

### Jaap
This has two major consequences...

### Jaap
1. Recursive self-improvement of hardware is harder than it is for software. 2.
Recursive self-improvement of a fixed-size bit-string is harder for more complicated
instruction sets.

### Mike
Interesting thought.

### Jaap
In other words, as you climb up the instruction set complexity (and usefulness I
might argue) hierarchy, you *lower* yourself down the ease-of-self-improvability
hierarchy.

### Jaap
I don't think I have a formal proof of this, but instinctively I think it's easier
to analyse and evolve Turing machine programs than it is to analyse and evolve x86
strings of the same length.

### Jaap
A supporting thought for this notion is that proper analysis of x86 bit strings
would require you to fill the address space with data about x86 definitions.

### Jaap
Let's see if I can put some bogus numbers on this to give an idea....

### Jaap
Address space is 1000.

### Jaap
Turing machine recursively self-improving program: 200 for the current program,
10 for storing how the host turing machine works, 790 left to work with.

### Jaap
x86 machine recursively self improving program: 20 for the current program, 580
for storing how an x86 host machine works, 400 left to work with.

### Mike
:)

### Jaap
Running speed of Turing machine, slow. Running speed of x86 machine, fast. Self-improvement
speed of Turing machine, small changes often. Self-improvement speed of x86 machine,
??? changes less often.

### Jaap
There's lots of holes in this idea I'm sure; and the numbers are bullcrap. I'm just
trying to illustrate a curious thought here, which is that increases in computing-efficiency
decrease computing-introspectability.

### Jaap
Hang on, I worded that poorly.

### Jaap
I mean increases in complexity of models for computing.

### Jaap
Sorry, I'm rambling on...

### Mike
:)

### Jaap
If you had a webcam I'd probably see you make the "blah blah blah" hand gesture
to Elisa.

### Mike
I suspect that we'll see AI more or less when it's inevitable.

### Mike
Which is to say...

### Mike
The factors we're talking about here would be made moot by an N-times increase in
computational speed, for some N.

### Mike
Looking 20 years in the future, following Moore, I've read that supercomputers should
have 10,000 times the "computing power" of the brain. Forgetting for a moment that
I have no idea how this is defined...

### Mike
I think differences in definition, too, will be absorbed by a sufficient increase
in computational power.

### Mike
So, to me, 20 to 30 years seems like the timeline. I suspect we will first see AI
simply as a result of a self-improving system run on a sufficiently powerful computer.

### Jaap
That's an Ad-Vis-Moores-Brutus argument.

### Mike
Maybe someone develops a system for finding faces for cameras, and it's *particularly*
good.

### Mike
"Ad-Vis-Moores-Brutus"

### Mike
:)

### Mike
I really think the main problem right now is a lack of horsepower.

### Mike
And perhaps architecture, but that, too, is heading in the right direction.

### Jaap
You don't think that ultimately (in hindsight, and possibly not understandable by
humans) there'll sit some fairly "trivial" (meaning elegant) idea underneath recursive
self-improvement?

### Jaap
Even if not invented originally as such by humans, at least at some point reduced
to that by the AI itself?

### Jaap
Not to say that that one particular idea would be the "only" way to do intelligence,
just that it would be one particular idea.

### Mike
Sure... I think that after the initial creation of AI, we should see refinement
of the concept.

### Jaap
The same way that evolution is if you step back far enough, is actually pretty damn
simlpe.

### Jaap
And "just another way" of creating intelligence.

### Mike
Right. That said, I don't think we'll ever look back and thing, "If only we had
the right idea in 2009."

### Jaap
A rather inefficient one I suspect, but arguably so far the only one that has been
shown to work... :)

### Mike
It'd be like saying that if you just had the right body, you could win a drag race
with a 5 HP lawn mower engine.

### Jaap
You don't think you could possibly contribute to this idea? Or it doesn't interest
you enough to pursue that?

### Mike
Sure, the science has been refined, but you still need a 1000 HP engine to win.

### Jaap
Jup, just like the science of neurons is refined, but you still need a gazillion
linked in an icky wet network before they're useful.

### Mike
:) Exactly. And, even at that, I'm sometimes surprised how limited human potential
is.

### Mike
(And, truth be told, heartened.)

### Jaap
Just like a need an atom clock on a sattelite and a large hadron collider to be
a physicist - which isn't the case....

### Jaap
I'm trying to figure out why you're less excited by this topic than I am.

### Mike
:) :)

### Mike
Well, I'm totally stoked by the idea, but I feel like right now our biggest limiting
factor is horsepower, and intuitively, I think the rest is non-critical.

### Mike
Which is to say...

### Jaap
I must admit, AI has been a long passion of mine - pretty much the oldest and strongest
for as long as I can remember touching computers. But I had my personal "AI winter"
after seeing too many if-then-else forests in video-games.

### Mike
Right.

### Jaap
Video-games are not AI, they're magic. Having been a fairly serious hobby magician
to the point of making money at weddings and birthday parties, I can really make
that analogy strongly. It's all smoke and mirrors.

### Mike
For me, I think, the problem is that I've done a bunch of stuff with genetic systems,
neural networks, and so on, and have always been slightly disappointed with the
possibilities.

### Jaap
But we've agreed on that before.

### Mike
Hey, good point.

### Mike
Re: magic.

### Jaap
I hear you on the neural network, genetics, etc. dissapointment.

### Jaap
But dude, have you seen http://www.nvidia.com/object/fermi_architecture.html?

### Jaap
Well, I don't have to tell you; you've had lunch with the Accelaware guys.

### Mike
Okay, I admit, CUDA seems really interesting to me.

### Mike
I haven't researched it enough, but was thinking the other night that it should
be high on my list.

### Mike
I am *very* excited by the present trend of massive parallelism.

### Jaap
The thing is, my gut tells me that some amalgamation of "dumb" ideas let loose on
Fermi^N is what'll thaw the AI winter.

### Jaap
And the minute it happens, that almagamation will turn out to be less than 10.000
lines of code executing on 10.000 cores.

### Jaap
10.000 LOC is doable for mortals...

### Mike
Agreed.

### Jaap
Why not you?

### Jaap
See, I have a good excuse. I don't want to destroy the world.

### Jaap
;), kidding.

### Mike
Well, at present, the lack of a major idea on how it might be done.

### Mike
But also, lack of study in CUDA.

### Mike
I have a simple idea which I think I will try implementing in the next 6 months
(not AI, just something from work that seems like a good toy parallel project).

### Mike
Wow, NVIDIA's doing a great job of putting some library support etc. behind this
concept.

### Jaap
Fair enough. I've been reading a lot on recent AI work (AIXI, OpenCog, Novamente,
Bayesian Blah Blah, etc.) and it has invigorated my delusions of AI grandeur again.

### Jaap
I know fairly certain I'm wrong, and the rate at which I skim instead of read papers
is a good indicator thereof; but there's a big fucking carrot dangling in front
of me and it's sooooo tempting.

### Mike
Added to my bookmark bar.

### Mike
Alright, time for me to go to sleep. Thanks for the ideas--will definitely be looking
into CUDA, at least.

### Jaap
Salut amigo. Thanks!

### Jaap
http://prefrontal.org/files/posters/Bennett-Salmon-2009.jpg

### Mike
:) I think I've seen this case before.

### Jaap
"The salmon was approximately 18 inches long, weighed 3.8 lbs, and was not alive
at the time of scanning."

### Jaap
LOL, so subtlely hidden...

### Mike
:) :)

### Mike
I'm having a really hard time getting work done in Cranbrook. I think a familiar
environment may be important.

### Mike
:P

### Jaap
Mweh, by that token I should be hyper productive this very second.

### Jaap
Then again, you list familiarity as a necessary, not a sufficient requirement.

### Mike
Well, not to say the converse is true.

### Mike
:) Yep.

### Mike
In fact, I think you've said it better than I did.

### Mike
This is not a "converse" thing at all. And in any case, the only sensible thing
would be an implication that the converse was false.

### Mike
Gah.

### Mike
Ah, the inverse, perhaps.

### Jaap
Tricky, there's a lot in your two initial sentences that can be either reverted
(converted) or inverted - without landing at the premise I posed.

### Jaap
Alas, details.

### Jaap
What are you trying to work on then, DirectAid stuff?

### Jaap
And why are you in Cranbrook again, or still? Your dad's health iirc?

### Mike
Elisa's dad died earlier this week. Funeral's on Tuesday.

### Mike
So we're hanging out until then.

### Mike
Came down just before to see him.

### Mike
Previously, he'd taken ill, so we came down to see him.

### Jaap
Ah, shitty. My condolences, please forward to Elisa.

### Mike
Will do.

### Jaap
Is that the first of one of your and Elisa's parents to pass away?

### Mike
Yep. Probably the hardest, too.

### Jaap
:( :( :(, really sorry mate.

### Mike
Elisa was really attached to her dad.

### Mike
Not that I'm not attached to my parents, but I *think* I take a slightly different
view on their deaths. I suppose we'll see, eventually.

### Jaap
I usually wouldn't bring this up in such a context [1], but I think you can handle
this; what are your thoughts on the possibility that Elisa's dad will have just
missed the boat in terms of longevity?

### Mike
:)

### Jaap
[1]: Meaning that I'm well aware that you don't say to a family member or friend
at the funeral: "boy, wish he had signed up for cryonics now, eh?"

### Mike
Well, I was thinking about that quite a lot. Two months ago, he was A-okay, and
then in the last 3 weeks or so, he just got taken down by an incredibly aggressive
lymphoma.

### Mike
Was quite interesting to watch.

### Jaap
Damn... how old?

### Mike
70.

### Mike
So, not super young, but not that old, either.

### Mike
But it made me think, at some point in the future (who knows how long), this would
be a non-issue.

### Mike
It's his freakin' body that died.

### Mike
And that's not really the part Elisa cared for.

### Jaap
Dude, you so totally nailed it. No Alzheimer's here, that guy had a fully functional
brain.

### Mike
It really underscored for me, I think, the extent to which we lose information when
someone dies like that.

### Mike
It's exactly like nobody made a backup, and then the machine was shut down. What
can you do?

### Mike
Well, you can make backups, for one.

### Jaap
I'll be curious if this event will change the strategy for you and/or Elisa, a little
ways down the right, once the immediacy of emotional and practical (e.g., funeral,
etc.) consequences has worn off a little.

### Jaap
"...down the right" -> "...down the way"

### Mike
An interesting question. For me, it doesn't quite reach the threshold of changing
things (emotional/practical matters being more or less complete for me).

### Mike
Trying to think why that is.

### Mike
Ooh...

### Mike
I saw something you will be interested in.

### Mike
http://en.wikipedia.org/wiki/St._Petersburg_paradox

### Mike
Thought of you when I read this, and here's why...

### Jaap
Yeah, I've been trying to figure out the disconnect between my convincement that
cryonics makes sense, and my paralysis towarsd actually signing up.

### Mike
It's one of these "paradoxes" that isn't really a paradox at all, just a bit of
a hiccup. But it's an interesting hiccup.

### Jaap
Probably a genetic handicap towards far-mode thinking, in favor of near-mode thinking.

### Mike
It's a game with infinite expected payoff, but which almost nobody would play.

### Mike
Well, that is, nobody would play it at a high price, despite the infinite expected
payoff.

### Jaap
I'm familiar with the St. Petersburg paradox, yes.

### Mike
Right. So...

### Mike
There are a couple of theories I've read on how it is resolved, and I'm not really
keen on either.

### Jaap
I spent most of October to better my game-theory knowledge...

### Mike
One is marginal utility.

### Mike
Bah.

### Jaap
....continue... I like where this is going.

### Mike
The other is risk-aversion. But they seem to frame it oddly...

### Mike
Looking for the place where I saw that one explained, but having a hard time finding
it. I really wasn't very happy with the explanation.

### Mike
In fact...

### Mike
A lot of the time I'm not really happy with this kind of discussion, because it
seems like a large number of the rebuttals are along the lines of, "Yeah, but a
casino could never make an infinite payout," which seems to be missing the point.

### Mike
Here's what I'm thinking...

### Mike
Suppose buy-in was a million dollars.

### Jaap
The risk-aversion explanation is that you'd run the risk of bottoming out your available
money.

### Mike
Expected payout is infinite, so we're still good. But, would you put a million on
the line for a very small (albeit profitable) chance to win more?

### Jaap
I think marginal utility is a ancestral genetic trait, related to near/far-mode
thinking.

### Mike
Ah, you know, risk aversion might be closer to what I think, then.

### Mike
It may have been poorly explained where I read it.

### Mike
To me, the marginal utility is not an issue.

### Mike
It's not that I wouldn't play because the payoffs are a small percentage of the
accumulated winnings.

### Mike
It's simply that if I have a million dollars in my pocket, I can think of better
ways to use it than to gamble it on winning 10 million.

### Jaap
I wouldn't play because the time/expected-pay-off ratio is poor. If I spent that
time working, I'll make more money.

### Mike
Sure.

### Mike
Coming around to the original point...

### Mike
With regard to cryonics, I think the problem for me is that, although the expected
payout is very high (arguably infinite), I stop short for similar reasons.

### Mike
The thing is, I'm really enjoying my life right now, and in order to invest in cryonics,
I would need to take something away from that.

### Jaap
But I think that's mostly because of a genetic blindspot towards putting value on
life extension.

### Mike
I don't think so. I put huge value on it.

### Mike
It is, to me, an infinite payout.

### Jaap
Yeah, but you don't "feel" it./

### Jaap
If you were told that you were going to die tomorrow.

### Mike
It's just an infinite payout *in the future* which I have to pay into now.

### Jaap
But pay some amount of money N to live another M days....

### Jaap
What would N and M be...?

### Mike
Hmm... An interesting question.

### Mike
Another M days after when?

### Mike
(I think this question itself may be more telling than the answer to it.)

### Jaap
After today (I know that isn't the case for cryonics).

### Mike
So, I'm going to die for sure tomorrow unless I shell out some cash.

### Jaap
The short-circuit argument to that is along the: "I don't negotiate with terrorists,"
because they're imaginging they're being mugged in a dark alley and have to pay
100 bucks...

### Mike
Are we bartering, or is the rate fixed?

### Mike
If we're bartering, then I'll give you $100 a day.

### Jaap
But that's not what I mean. Zoom out far enough and our bodies (/evolution) becomes
the terrorist.

### Mike
If the rate is fixed, then I'll give you my entire accumulated wealth divided by
that rate.

### Mike
Whoops.

### Mike
That is, I'll give you my entire accumulated wealth.

### Mike
At that rate.

### Mike
If I knew I was gong to die tomorrow if I didn't spend the money, then I think it
would be poinless to hang onto the cash, which won't do me any good when I'm wormfood.

### Jaap
So _if_ it was proven that Cryonics works (which is kinda silly, because the minute
that it gets proven, you don't have to sign up anymore - something that most people
don't realize), you'd pay a hundred bucks a day for its effects.

### Mike
That's my starting price, not my limit.

### Mike
But, it's also contingent on my knowing that, otherwise, I'll die tomorrow.

### Mike
If I don't know when I'm going to die, then for now, I'm inclined to play the odds.

### Jaap
Yeah, the mugging argument doesn't tranlate entirely. But it forces one to momentarily
think and realize: "wait a second, I don't want to die, I'd pay a shitload of money
(everything I have, because when I'm dead, it's not worth anything) to live another
day."

### Jaap
Most people never get to that realization....

### Mike
:)

### Jaap
So the odds you're playing are that life-extension will come while you are still
alive...

### Mike
More or less. I see a probability of my surviving N days.

### Jaap
Those are non-unrealistic odds, and they bring me to my current standing opinion
on the future - my most simplified but most probable belief on the singularity.

### Jaap
And that is....

### Jaap
"I will not die of old age."

### Mike
Indeed. More or less my problem, I think.

### Jaap
In other words: 1. I might die in a car accident or BASE jump the next 50 years.
2. Humanity may be whiped out. 3. Humanity may be transformed.

### Jaap
4. Everything will take way longer than we think/hope, in which case.... I might
die of old age.

### Mike
Not that I think there's a high chance I will die, e.g., BASE jumping, but were
it to happen, cryonics would be pointless.

### Jaap
What's your email address to use? Gmail?

### Mike
Jup.

### Jaap
Sent you an email.

### Mike
I certainly feel like the most likely way for me to die in the short term is major
trauma (car accident, BASE jump, shot in the head by a rogue AI, whatever).

### Mike
In the very long term, old age becomes an issue.

### Mike
But that gives me lots of time to prepare.

### Jaap
My email raises a point I recently thought of and one that I haven't seen before
explicitly.

### Mike
It's like the argument that I can maintain high-risk investments while I'm young,
because convergence of my invested sum is not critical.

### Mike
It's okay if it goes up/down 50%.

### Mike
When I get closer to retirement, that becomes an issue.

### Mike
So, my investment strategy changes.

### Jaap
Wow...

### Jaap
That may go for money, but not for life.

### Mike
Why not?

### Jaap
Or did I misinterpret?

### Mike
Well, not that high-risk is the way to go for life.

### Mike
But that strategy changes depending on the perceived risks.

### Jaap
Because if you have enough life, you can recover from poverty. But if you're out
of life, you can't recover.

### Mike
In the case of life, it's the idea that I am unlikely to die of old age in the next
10 years.

### Mike
Or, for that matter, from lymphoma.

### Jaap
That's why playing a long enough St. Petersburg isn't appealing to people, because
they can bottom out and go bankrupt (financial equivalent of dying, you can't recover
(within the game)).

### Jaap
Ok, let me summarize.

### Jaap
The statement: "...because I'm young, I can BASE jump, because I haven't invested
a whole lot yet." That's true from an outside view (similar to how a gambler has
an outside view onto his wallet with money)....

### Mike
Ah, not quite what I meant.

### Mike
(Though perhaps you are not referring to my statements.)

### Jaap
You mean, "...because I'm young, the kind of death where cryonics may help is not
likely enough compared to the other future outcomes (e.g., we invent longevity,
AI's whipe us out, I die from BASE) that it makes it worth the money.

### Mike
Or the cognitive load, really.

### Jaap
And that I'm inclined to agree with... Cryonics itself is already a scientific gamble,
and you're adding a personal gamble on top, that you'll reach death in such a way
that it'll be amenable to cryonics.

### Jaap
Yeah, agreed. The cognitive load is a bitch. Filling out forms, signing paper work,
talking to friends/family, faxing shit.

### Jaap
And I'm not even being sarcastic...

### Mike
:)

### Mike
I figured not.

### Jaap
This guy is useful though: http://www.rudihoffman.com/cryonics_non_us.html

### Jaap
But alas, I totally get your point, and I agree.

### Jaap
My "wow" was to my initial (mistaken) interpretation that you translated a gambling
analogy to life. Utility is cool and all, but I am not the dollar bill in my wallet.
And corollary; I'm glad my dollar bill isn't conscious.

### Mike
Agreed 100%.

### Jaap
Did you get my email? Wondering if you have any thoughts on the "wanna be part of
the rapture" argument.

### Mike
From the page you linked: "In order for me to work with you to obtain this affordable,
high quality coverage, however, the big requirement is this: YOU must be highly
motivated, totally committed to following through on what is requested of you, and
FUN and COOPERATIVE to work with."

### Mike
This turns me off quite a lot. Makes it sound like the guy is doing me some kind
of favour by offering this.

### Jaap
It came up for me when I thought about AI and Friendly AI. It seems a great deal
of people acknowledge the danger, and then run along headfirst anyway. I'm one of
them, I can't wait to whip up some OpenCL on an NVIDIA Fermi and whipe out humanity.

### Mike
I hate when people make business sound like a favour. Makes my skin crawl.

### Jaap
:D

### Mike
Re: email, didn't see a part about rapture. Did I miss something?

### Jaap
Really? Interesting, I had the complete opposite reaction. It made me like the guy
better.

### Jaap
I think in that line of business, he works with a lot of people who underestimate
the complexity.

### Mike
But then I expect him to charge more.

### Mike
Not make personal requirements.

### Mike
:)

### Jaap
His use of ALL CAPS is poor, but the content is not.

### Mike
Is it possible I'm more of a capitalist than you are?

### Mike
;)

### Jaap
By telling me what he expects from me, he gives me the right to expect things from
him.

### Mike
Yeah, but I don't want to expect anything of him, except what's in the contract.

### Mike
And that's business.

### Mike
:)

### Jaap
I've just worked in large companies longer than you, and I know that people are
not perfect rational agents. When I talk to somebody on the phone, especially when
it's regarding my death; I want to talk to somebody with a sense of humour and friendliness.

### Jaap
By expressing that he expects the same from me, he's given me the impression that
he operates like I do.

### Mike
(All that said, the rest of what the guy says sounds fine to me, so I would be inclined
to say he doesn't mean that last point in the way I'm inclined to take it.)

### Jaap
Well, I think he gets a lot of cheap-skates. People who want to sign up once they
hear they've got terminal cancer and only 3 months to go....

### Jaap
I would imagine a healthy 29 year old is much easier to deal with...

### Mike
Re: email/rapture, are you referring to the transformational event?

### Jaap
Alas, I'm not defending him. Just pointing out that my reaction to that statement
was the complete opposite than yours. And not just after you pointed it out to me
either.

### Jaap
I remember noticing the statement when I first read it. And when you quoted it,
I expected you were going to point out that you liked it.

### Jaap
Guess we're not clones after all... :D

### Jaap
Ok, rapture.

### Jaap
Quoting from my email...

### Jaap
"Namely, that if my life has to end anyway, Id rather that happens as part of a
big interesting transformational (if not catastrophic) event, than it happening
as just another mundane death like the majority of deaths that have occurred before
mine."

### Mike
Roger.

### Jaap
It seems more fun to die when the world ends (i.e., the end station for the species),
than somewhere along the middle....

### Mike
Hmmm...

### Mike
A couple of thoughts come to mind.

### Jaap
Let's say I had a button I could push to make the singularity happen, I was going
to die in 3 months, and I knew for a fact that the singularity _will_ happen in
3 decades. I also know that if I push the button now, 2 billion people will die,
whereas if I don't, a delayed singularity might kill fewer people.

### Jaap
What would I do?

### Mike
1. I'm not completely sure I think it would be more interesting to die in a revolution
than of old age. I may think it would be more interesting to *live* through a revolution...
Although, even there, at the moment I'm inclined to say I'd rather have a choice
of how I spend my time, as opposed to being forced to take up arms, etc.

### Jaap
I think it'd be quite excited to push that button. Not even to save my life, but
just to make the last 3 months more exciting.

### Mike
2. I wonder what effect this perception has on my interest in, e.g., cryonics.

### Mike
lol.

### Mike
(Just read what you wrote.)

### Jaap
Increase the viability of cryonics, and I'd be more inclined to let it sit. The
last thing you want to do before getting frozen is piss off a lot of people.

### Jaap
I think my motivation boils down to a core motivation of my life...

### Jaap
Which is to figure out what the hell is going on... I.e., peek outside the universe,
and have a look-see what put it there.

### Jaap
I don't give myself much of a shot to actually accomplish that, but I like to think
that somewhere in the universe there's a convergence of intelligence that's overcoming
entropy towards trillion years from now Omega point.

### Jaap
Maybe our species is a step along the way to that. Or maybe we're a dead-end branch...\

### Mike
:) :)

### Jaap
Either way, we're part of something, and I want to get the most out of it. Just
to satisfy my own curiousity, nothing else.

### Jaap
You can sort of see it as a bandwidth issue...

### Jaap
The more shit happens before I die, the more stuff I can cram through my own IO
ports, the closer I get to the final point.

### Jaap
We're talking about moving a millimeter forward on a lightyear long scale, but I
got this thing called curiousity, and it's mighty powerful.

### Jaap
So who knows, I might push that button.

### Mike
:)

### Jaap
:D

### Jaap
Unless I had signed up for cryonics, per my comment above. Don't want to piss off people, right?
