---
layout: post
title: "Singularity Shrugs"
---

# {{ page.title }}

### Jaap
You know, I'm accutely aware of how much I'm seduced by this flavor of futurism. I can think of 
two reasons for this.

The first is that the singularity is a great excuse to avoid the present. Why care about the dishes, if the world is going to end tomorrow?

Secondly, my record shows I have built-in desire to look for (what I at a particular point consider) the rational minority. It's not always
clear how much of that is just to be contrary for contrary's sake, and how much is a genuine and sincere desire to learn. 

### Mike
    
Ack... :-)

### Jaap
Now my awareness of this, makes me pursue counter-singularity points. So far, I'm not having much
luck finding compelling cases. But again, perhaps that is because of my own bias, right

The most compelling "*it's not going to happen*" articles are the ones that rely on
arguments about complexity and/or unpredictability.

Meaning; it's not going to happen anytime soon, and/or stuff may or will happen,
but it'll almost surely not be among the cases that we thought it would be, so why
bother.

Any thoughts on my concern? Let me know if you're busy...

### Mike
No, I have a few minutes at least. Heading out with Jason and
Carmen sometime, but waiting on their call. Otherwise, just surfing
Haskell stuff.

### Jaap
I suppose that we're BASE jumpers and still alive lends some credence to the fact
that we can approach minority views without being idiots.

### Mike
Nice... :)

Referring to the singularity as another minority view, I assume.

### Jaap
Yes.

### Mike
Well... my lack of concern is fueled mainly by the argument of resources.

It seems to me that real-world acquisition of resources slows things down *a lot*.

Or perhaps it's just a lack of imagination.

### Jaap
Oh, wait - you're talking about **FOOM**. I should clarify; I've broadened the
topic from the specific risk of AI whiping out the species to any and all consequences
of some kind of singularity (such as nanotech, robotics, uploading, longevity, etc.)

### Mike
Ah, I see...     :) :)

Roger.

In that case...

I think I have an interest in those things, but I tend to "*float*" it until it seems
more immediately relevant.

For the moment, there are some things that are near the threshold, like cryonics.

Other things, like uploading, are interesting, but not quite on the radar for me.

AI, in the broader sense (not in the going-to-kill-us-all sense) is intriguing to
me. As is computation in a non-sequential manner.

These are things that *are* on my radar.

But I think it's safe to say I have a pretty tight "*relevance*" filter, in some ways.

### Mike
I have a feeling the singularity itself will also be slowed somewhat by resource
acquisition.

Wow, reminds me of diodes, actually.

An ideal diode has a current-voltage characteristic something like *I = e^V*. A diode in series with a
resistor, even a small one, is dominated by the resistance as *V* increases.

With the singularity, I feel like right now we're still in the "ideal" region. Things
look exponential.

But I think the time it takes to do things in the real world, and to acquire resources,
will prevent a "true" singularity - in the mathematical sense.

### Jaap
Interesting analogy - though I must admit my Maxwell's equations are a little rusty
to say if I agree. The domination point seems valid, but really we're just talking
laws of physics. Like a resistor, an AI would have to obey as well.

I wouldn't completely discount the possibility of AI going FTL, but if the speed of light
turns out to be a critical ingredient for Friendly AI; then I don't think it'll be the
breaking point in the end - too many other problems closer to home.

What I'm really trying to get your thoughts on, is more a public discourse kind of thing. I
have a sense there's a dissonance between my interest in the topic, its weight, and how
skeptical and/or indifferent the majority view is.

Consider a parallel universe where suddenly overnight tomorrow, everybody would
actually agree that it's coming, and that it's coming soon. Still permitting some
unpredictability, but really just variations on a theme/blend of Kurzweil, Yudkowsky, Hanson,
Bostrom, Goertzel, Sandberg, De Grey, et al.

Would you not imagine that things would look a little different?

### Mike
(Reading...)

Well, sure it would look different. But if you imagine a parallel universe populated
by unicorns (because I know you do), that would look different too. I guess the
question is, what of the world where everyone believes the singularity is approaching?

### Jaap
True singularity in the omega point sense. Aside from the potential existential
risk, that topic does remain firmly in the "*interesting*" part of my brain's bookshelf.
Meanwhile, topics like cryonics, uploading, AI (in the less fatalistic sense), etc.
are creeping away from that corner of my brain towards my "*factors included in my
near-mode decision making process*" part of the brain.

 * **Unicorns are far less likely than singularities.**

You ask; what of that world? I answer...

That world might behave in a way that helps me evaluate better how I should
make decisions in my world.

If the whole world in 1935 knew for a fact that there would be another world war in
full swing by 1942 (or 39, or 40, depending), I think a large subset of those
people would have made a few decisions differently than they actually did.

You might say; but the world didn't know that...

### Mike
Fair enough. I think the problem, from my standpoint, is that the singularity is,
basically by definition, unknowable. And if we can't know it, then we are spending
energy preparing for a very speculative future.

### Jaap
Sure, and given that we haven't been to 2057 yet, nobody knows what'll actually
happen. But that doesn't change the fact that probabilistic assesments/beliefs about
possible future outcomes (be it on a five minute scale or a five decade scale) *do*
affect our decision making; even that of the most idiotic person.

### Mike
Speculating about that future, and trying to work out probabilities, is interesting,
but not to the point that it changes my actions today.

Simply because the space of probabilities ahead of us is so broad.

Committing to one just because "something is coming" would be a bit like Pascal's
wager.

There's opportunity cost in the present.

### Jaap
I guess I'm asking whether or not you agree that there is a dissonance between the
actual public's awareness on the topic, and the effect the possible significance
of a future outcome could have on the public today.

### Mike
On the five minute/five year scale, I feel like I have a handle on what will happen.

On the fifty year scale, not so much.

### Jaap
True enough...

### Mike

I think if we *knew* what would happen, or even had a reasonably good idea, then
it might affect things. But even basic propositions like, "*AI will eventually supercede
us*" seem slightly questionable to me.

There are so many factors that could prevent that from occuring as we see it, so
it seems a bit pointless to prepare for one particular vision of it.

Gotta run. Back later, though, and have really been enjoying these chats.

TTYL!

### Jaap
Cya!
